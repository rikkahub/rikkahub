"""
RikkaHub Sandbox Tool - å®‰å…¨çš„æ–‡ä»¶ç³»ç»Ÿæ“ä½œå’Œ Python æ‰§è¡Œç¯å¢ƒ

ã€é‡è¦é™åˆ¶ã€‘
Chaquopy ä¸æ”¯æŒè¿è¡Œæ—¶å®‰è£… Python åŒ…ã€‚ä½¿ç”¨ list_available_packages æŸ¥çœ‹æ‰€æœ‰é¢„è£…åŒ…ã€‚

ã€å¯ç”¨å·¥å…·åˆ†ç±»ã€‘
æ–‡ä»¶æ“ä½œ: read, write, list, delete, mkdir, copy, move, stat, exists
å‹ç¼©æ–‡ä»¶: unzip, zip_create
ä»£ç æ‰§è¡Œ: python_exec, exec, exec_script
æ•°æ®å¤„ç†: process_image, convert_excel, extract_pdf_text, download_file
æ•°æ®åº“: sqlite_query, sqlite_tables
ç‰ˆæœ¬æ§åˆ¶: git_init, git_status, git_log, git_diff, git_add, git_commit, git_branch, git_checkout, git_rm, git_mv
å¤šè¯­è¨€: exec_js, exec_lua
ä»£ç åˆ†æ: analyze_code, compile_check
åŒ…ç®¡ç†: list_available_packagesï¼ˆæŸ¥çœ‹é¢„è£…åŒ…ï¼‰
å·¥å…·å®‰è£…: install_tool

ã€ç¤ºä¾‹ã€‘
{
    "operation": "python_exec",
    "code": "import numpy as np\nprint(np.sum([1,2,3]))"
}
"""

import os
import zipfile
import subprocess
import json
import shutil
import shlex
import base64
from typing import Dict, Any, List, Optional
from pathlib import Path

# Android ç¯å¢ƒä¸‹çš„å‘½ä»¤è·¯å¾„
ANDROID_PATHS = [
    '/system/bin',
    '/system/xbin',
    '/vendor/bin',
    '/data/data/com.termux/files/usr/bin',  # Termux (å¦‚æœå­˜åœ¨)
]

def _get_android_command(cmd: str) -> str:
    """åœ¨ Android ç¯å¢ƒä¸­æŸ¥æ‰¾å‘½ä»¤å®Œæ•´è·¯å¾„"""
    # å¦‚æœå·²ç»æ˜¯å®Œæ•´è·¯å¾„ï¼Œç›´æ¥è¿”å›
    if os.path.isabs(cmd) and os.path.exists(cmd):
        return cmd
    
    # åœ¨å¸¸è§è·¯å¾„ä¸­æŸ¥æ‰¾
    for path in ANDROID_PATHS:
        full_path = os.path.join(path, cmd)
        if os.path.exists(full_path):
            return full_path
    
    # è¿”å›åŸå‘½ä»¤ï¼Œè®©ç³»ç»Ÿå°è¯•è§£æ
    return cmd

def _to_json_serializable(obj: Any) -> Any:
    """å°† Python å¯¹è±¡è½¬ä¸º JSON å¯åºåˆ—åŒ–æ ¼å¼"""
    if isinstance(obj, dict):
        return {k: _to_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_to_json_serializable(v) for v in obj]
    elif isinstance(obj, (int, float, str, bool)):
        return obj
    elif obj is None:
        return None
    else:
        return str(obj)

# å‘½ä»¤ç™½åå• - å…è®¸æ‰§è¡Œçš„åŸºç¡€å‘½ä»¤
# åŸºäº Android Toybox å®é™…å¯ç”¨çš„å‘½ä»¤
ALLOWED_COMMANDS = {
    # === æ–‡ä»¶æ“ä½œ (Toybox å¯ç”¨) ===
    'ls', 'cat', 'grep', 'sed', 'awk', 'head', 'tail',
    'cp', 'mv', 'rm', 'mkdir', 'touch', 'chmod',
    'find', 'which', 'ln', 'readlink', 'realpath',
    'du', 'df', 'stat', 'file', 'basename', 'dirname',
    
    # === å‹ç¼©è§£å‹ (Toybox å¯ç”¨) ===
    'tar', 'gzip', 'gunzip', 'bzip2', 'bunzip2', 'lzma',
    # æ³¨æ„: unzip, zip, xz å¯èƒ½ä¸å¯ç”¨
    
    # === æ–‡æœ¬å¤„ç† (Toybox å¯ç”¨) ===
    'sort', 'uniq', 'wc', 'cut', 'tr', 'diff', 'patch',
    'tee', 'split', 'csplit', 'comm', 'nl', 'fmt', 'pr', 'fold',
    'rev', 'tac', 'hexdump', 'od', 'strings',
    # æ³¨æ„: join å¯èƒ½ä¸å¯ç”¨
    
    # === ç½‘ç»œå·¥å…· (éƒ¨åˆ†å¯ç”¨) ===
    'curl', 'ping', 'wget',  # curl é€šå¸¸å¯ç”¨ï¼Œwget å¯èƒ½ä¸å¯ç”¨
    
    # === ç³»ç»Ÿä¿¡æ¯ (Toybox å¯ç”¨) ===
    'echo', 'pwd', 'whoami', 'date', 'uname', 'env', 'printenv',
    'id', 'groups', 'uptime', 'hostname',
    
    # === è¿›ç¨‹ç®¡ç† (Toybox å¯ç”¨) ===
    'ps', 'pgrep', 'pkill',
    # æ³¨æ„: ps ä½¿ç”¨ Toybox è¯­æ³• (ps -A è€Œé ps aux)
    
    # === ç¼–è¾‘å™¨/æŸ¥çœ‹å™¨ (éƒ¨åˆ†å¯ç”¨) ===
    'vi', 'more',
    # æ³¨æ„: vim, nano, less ä¸å¯ç”¨
}

# ç”¨æˆ·é€šè¿‡ pip å®‰è£…çš„ Python åŒ…å‘½ä»¤ç™½åå•
# è¿™äº›æ˜¯å¸¸ç”¨çš„æ•°æ®å¤„ç†/åˆ†æå·¥å…·
PIP_COMMAND_ALLOWLIST = {
    # æ•°æ®å¤„ç†
    'jq',  # JSON å¤„ç†
    'yq',  # YAML å¤„ç†  
    'csvkit',  # CSV å·¥å…·é›†
    'xmltodict',  # XML è½¬æ¢
    
    # æ–‡æœ¬å¤„ç†
    'pygments',  # è¯­æ³•é«˜äº®
    'markdown',  # Markdown å¤„ç†
    
    # ä»£ç å·¥å…·
    'black', 'yapf', 'autopep8',  # Python æ ¼å¼åŒ–
    'pylint', 'flake8', 'mypy',  # Python æ£€æŸ¥
    
    # å®ç”¨å·¥å…·
    'httpie',  # HTTP å®¢æˆ·ç«¯
    'http-prompt',  # HTTP äº¤äº’
    'xh',  # HTTPie æ›¿ä»£
    
    # æ–‡ä»¶å¤„ç†
    'chardet',  # ç¼–ç æ£€æµ‹
    'file-magic',  # æ–‡ä»¶ç±»å‹æ£€æµ‹
}

# å·²å®‰è£…çš„ pip å‘½ä»¤ç¼“å­˜
_installed_pip_commands = set()

# é»‘åå•å…³é”®å­— - ç»å¯¹ç¦æ­¢çš„æ“ä½œ
BLOCKED_KEYWORDS = [
    # ç¼–è¯‘æ„å»ºç›¸å…³
    'javac', 'kotlin', 'gradle', 'ndk-build', 'make', 'cmake', 'gcc', 'g++', 'clang',
    # åŒ…ç®¡ç†å™¨
    'apt', 'apt-get', 'yum', 'dnf', 'pacman', 'brew', 'pkg',
    # ç³»ç»Ÿçº§å±é™©å‘½ä»¤
    'su', 'sudo', 'mount', 'umount', 'mkfs', 'fdisk', 'dd',
    'mkfs.ext', 'mkfs.ntfs', 'format',
    # æƒé™æå‡
    'chmod 777', 'chmod +s', 'chown root',
    # æ•æ„Ÿè·¯å¾„
    '/system', '/proc', '/sys', '/dev',
]

# æœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶ (50MB)
MAX_FILE_SIZE = 50 * 1024 * 1024

# å‘½ä»¤æ‰§è¡Œè¶…æ—¶ (30ç§’)
COMMAND_TIMEOUT = 30


def execute(operation: str, sandbox_path: str, params: Dict[str, Any]) -> str:
    """
    ä¸»å…¥å£å‡½æ•° - æ‰§è¡Œæ²™ç®±æ“ä½œ
    
    Args:
        operation: æ“ä½œç±»å‹
        sandbox_path: æ²™ç®±æ ¹ç›®å½•è·¯å¾„
        params: æ“ä½œå‚æ•°
    
    Returns:
        JSON å­—ç¬¦ä¸²æ ¼å¼çš„æ“ä½œç»“æœ
    """
    try:
        # æ ‡å‡†åŒ–è·¯å¾„
        sandbox_path = os.path.normpath(os.path.abspath(sandbox_path))
        
        # éªŒè¯æ²™ç®±ç›®å½•å­˜åœ¨
        if not os.path.exists(sandbox_path):
            os.makedirs(sandbox_path, exist_ok=True)
        
        # æ ¹æ®æ“ä½œç±»å‹åˆ†å‘
        handlers = {
            "unzip": _unzip_file,
            "zip_create": _create_zip,
            "exec": _exec_command,
            "list": _list_files,
            "read": _read_file,
            "write": _write_file,
            "delete": _delete_file,
            "mkdir": _make_directory,
            "copy": _copy_file,
            "move": _move_file,
            "stat": _file_stat,
            "exists": _file_exists,
            "python_exec": _python_exec,
            # ä¾¿åˆ©æ“ä½œ
            "process_image": _process_image,
            "convert_excel": _convert_excel,
            "extract_pdf_text": _extract_pdf_text,
            "download_file": _download_file,
            "sqlite_query": _sqlite_query,
            "sqlite_tables": _sqlite_tables,
            # Git æ“ä½œ
            "git_init": _git_init,
            "git_status": _git_status,
            "git_log": _git_log,
            "git_diff": _git_diff,
            "git_add": _git_add,
            "git_commit": _git_commit,
            "git_branch": _git_branch,
            "git_checkout": _git_checkout,
            "git_rm": _git_rm,
            "git_mv": _git_mv,
            # Workflow checkpoint operations
            "git_checkpoint": _git_checkpoint,
            "git_restore": _git_restore,
            "git_list_checkpoints": _git_list_checkpoints,
            # Shell è„šæœ¬
            "exec_script": _exec_script,
            # å¤šè¯­è¨€æ”¯æŒ
            "exec_js": _exec_javascript,
            "exec_lua": _exec_lua,
            "analyze_code": _analyze_code,
            # ç¼–è¯‘éªŒè¯
            "compile_check": _compile_check,
            # å·¥å…·å®‰è£…
            "install_tool": _install_tool,
            # åŒ…ç®¡ç†ï¼ˆåŸºäº Chaquopy é¢„è£…ç¯å¢ƒï¼‰
            "list_available_packages": _list_available_packages,
        }
        
        handler = handlers.get(operation)
        if not handler:
            result = {
                "success": False,
                "error": f"Unknown operation: {operation}",
                "available_operations": list(handlers.keys())
            }
            return json.dumps(result)
        
        result = handler(sandbox_path, params)
        return json.dumps(_to_json_serializable(result))
        
    except Exception as e:
        import traceback
        result = {
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }
        return json.dumps(result)


def _validate_path(sandbox_path: str, user_path: str) -> str:
    """
    éªŒè¯å¹¶è§„èŒƒåŒ–ç”¨æˆ·æä¾›çš„æ–‡ä»¶è·¯å¾„
    é˜²æ­¢ç›®å½•éå†æ”»å‡»
    """
    if user_path is None:
        raise ValueError("Path cannot be None")
    
    # è§„èŒƒåŒ–è·¯å¾„
    full_path = os.path.normpath(os.path.join(sandbox_path, user_path))
    
    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è·¯å¾„åœ¨æ²™ç®±å†…
    if not full_path.startswith(os.path.normpath(sandbox_path)):
        raise ValueError(f"Path traversal detected: {user_path}")
    
    return full_path


def _is_safe_command(command: str) -> tuple[bool, str]:
    """
    æ£€æŸ¥å‘½ä»¤æ˜¯å¦å®‰å…¨
    è¿”å›: (æ˜¯å¦å®‰å…¨, é”™è¯¯ä¿¡æ¯)
    """
    if not command or not isinstance(command, str):
        return False, "Command must be a non-empty string"
    
    command_lower = command.lower()
    
    # æ£€æŸ¥é»‘åå•å…³é”®å­—
    for keyword in BLOCKED_KEYWORDS:
        if keyword.lower() in command_lower:
            return False, f"Command contains blocked keyword: {keyword}"
    
    # è§£æå‘½ä»¤
    try:
        args = shlex.split(command)
    except ValueError as e:
        return False, f"Invalid command syntax: {e}"
    
    if not args:
        return False, "Empty command"
    
    cmd = args[0]
    
    # æ£€æŸ¥æ˜¯å¦åœ¨ç™½åå•
    if cmd in ALLOWED_COMMANDS:
        return True, ""
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å·²å®‰è£…çš„ pip å‘½ä»¤
    if cmd in _installed_pip_commands:
        return True, ""
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯é¢„æˆæƒçš„ pip åŒ…å‘½ä»¤
    if cmd in PIP_COMMAND_ALLOWLIST:
        # è‡ªåŠ¨æ·»åŠ åˆ°å·²å®‰è£…ç¼“å­˜ï¼ˆå³ä½¿è¿˜æ²¡å®‰è£…ï¼Œå…è®¸å°è¯•æ‰§è¡Œï¼‰
        _installed_pip_commands.add(cmd)
        return True, ""
    
    return False, f"Command '{cmd}' is not in whitelist. Allowed system commands: {', '.join(sorted(ALLOWED_COMMANDS))}. You can install additional tools using install_tool operation."


def _unzip_file(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """è§£å‹ ZIP æ–‡ä»¶"""
    zip_path = params.get('zip_path')
    target_dir = params.get('target_dir', '.')
    
    if not zip_path:
        return {"success": False, "error": "Missing required parameter: zip_path"}
    
    full_zip_path = _validate_path(sandbox_path, zip_path)
    full_target_dir = _validate_path(sandbox_path, target_dir)
    
    if not os.path.exists(full_zip_path):
        return {"success": False, "error": f"ZIP file not found: {zip_path}"}
    
    if not zipfile.is_zipfile(full_zip_path):
        return {"success": False, "error": f"Not a valid ZIP file: {zip_path}"}
    
    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
    os.makedirs(full_target_dir, exist_ok=True)
    
    extracted_files = []
    with zipfile.ZipFile(full_zip_path, 'r') as zf:
        # æ£€æŸ¥ Zip Slip
        for member in zf.namelist():
            member_path = os.path.normpath(os.path.join(full_target_dir, member))
            if not member_path.startswith(os.path.normpath(full_target_dir)):
                return {
                    "success": False, 
                    "error": f"Zip Slip attack detected in file: {member}"
                }
        
        # è§£å‹
        zf.extractall(full_target_dir)
        extracted_files = zf.namelist()
    
    # Generate human-readable stdout
    file_list = []
    for f in extracted_files[:20]:
        icon = "ğŸ“" if f.endswith("/") else "ğŸ“„"
        file_list.append(f"  {icon} {f}")
    if len(extracted_files) > 20:
        file_list.append(f"  ... and {len(extracted_files) - 20} more files")
    stdout_lines = [f"ğŸ“¦ è§£å‹æ–‡ä»¶: {zip_path}", f"ğŸ“‚ ç›®æ ‡ç›®å½•: {target_dir}", f"ğŸ“Š å…±è§£å‹ {len(extracted_files)} ä¸ªæ–‡ä»¶", ""]
    if file_list:
        stdout_lines.extend(file_list)
    else:
        stdout_lines.append("  (ç©ºå‹ç¼©åŒ…)")
    stdout = "\n".join(stdout_lines)
    return {
        "success": True,
        "data": f"Extracted {len(extracted_files)} files to {target_dir}",
        "stdout": stdout,
        "files": extracted_files,
        "target_dir": target_dir
    }


def _create_zip(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ›å»º ZIP æ–‡ä»¶"""
    source_paths = params.get('source_paths', [])
    zip_name = params.get('zip_name')
    
    if not zip_name:
        return {"success": False, "error": "Missing required parameter: zip_name"}
    
    if not source_paths:
        return {"success": False, "error": "Missing required parameter: source_paths"}
    
    # ç¡®ä¿ zip åä»¥ .zip ç»“å°¾
    if not zip_name.endswith('.zip'):
        zip_name += '.zip'
    
    zip_path = _validate_path(sandbox_path, zip_name)
    
    created_files = []
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
        for source in source_paths:
            full_source = _validate_path(sandbox_path, source)
            
            if os.path.isfile(full_source):
                arcname = os.path.basename(source)
                zf.write(full_source, arcname)
                created_files.append(arcname)
            elif os.path.isdir(full_source):
                for root, dirs, files in os.walk(full_source):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, sandbox_path)
                        zf.write(file_path, arcname)
                        created_files.append(arcname)
    
    file_size = os.path.getsize(zip_path)
    
    return {
        "success": True,
        "data": f"Created {zip_name} ({file_size} bytes)",
        "file_name": zip_name,
        "file_size": file_size,
        "files_count": len(created_files),
        "file_path": zip_name  # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œè®© App é€šè¿‡ FileProvider ç”Ÿæˆ URI
    }


def _exec_command(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡Œå‘½ä»¤ (é€‚é… Android ç¯å¢ƒ)"""
    command = params.get('command')
    
    if not command:
        return {"success": False, "error": "Missing required parameter: command"}
    
    # å®‰å…¨æ£€æŸ¥
    is_safe, error_msg = _is_safe_command(command)
    if not is_safe:
        return {"success": False, "error": error_msg}
    
    try:
        args = shlex.split(command)
        
        # åœ¨ Android ä¸ŠæŸ¥æ‰¾å‘½ä»¤å®Œæ•´è·¯å¾„
        if args:
            cmd = _get_android_command(args[0])
            args[0] = cmd
        
        # è®¾ç½®å—é™ç¯å¢ƒ (Android å…¼å®¹)
        env = os.environ.copy() if 'os' in dir() else {}
        env['HOME'] = sandbox_path
        env['PWD'] = sandbox_path
        env['TMPDIR'] = os.path.join(sandbox_path, '.tmp')
        
        # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
        os.makedirs(env['TMPDIR'], exist_ok=True)
        
        # Android ä¸Šé€šå¸¸ä½¿ç”¨ /system/bin/sh
        shell_path = '/system/bin/sh'
        if os.path.exists(shell_path):
            # é€šè¿‡ sh -c æ‰§è¡Œï¼Œæ›´å…¼å®¹ Android
            full_cmd = ' '.join(shlex.quote(arg) for arg in args)
            result = subprocess.run(
                [shell_path, '-c', full_cmd],
                cwd=sandbox_path,
                capture_output=True,
                text=True,
                timeout=COMMAND_TIMEOUT,
                env=env
            )
        else:
            # ç›´æ¥æ‰§è¡Œ
            result = subprocess.run(
                args,
                cwd=sandbox_path,
                capture_output=True,
                text=True,
                timeout=COMMAND_TIMEOUT,
                env=env
            )
        
        return {
            "success": result.returncode == 0,
            "stdout": result.stdout[:10000] if result.stdout else "",  # é™åˆ¶è¾“å‡ºå¤§å°
            "stderr": result.stderr[:5000] if result.stderr else "",
            "returncode": result.returncode
        }
        
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": f"Command timeout (max {COMMAND_TIMEOUT}s)",
            "stdout": "",
            "stderr": "",
            "returncode": -1
        }
    except FileNotFoundError as e:
        return {
            "success": False,
            "error": f"Command not found: {str(e)}. Note: Many Linux commands are not available on Android.",
            "stdout": "",
            "stderr": str(e),
            "returncode": -1
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"Execution failed: {str(e)}",
            "stdout": "",
            "stderr": str(e),
            "returncode": -1
        }


def _list_files(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ—å‡ºç›®å½•å†…å®¹"""
    path = params.get('path', '.')
    show_hidden = params.get('show_hidden', False)
    
    full_path = _validate_path(sandbox_path, path)
    
    if not os.path.exists(full_path):
        return {"success": False, "error": f"Path not found: {path}"}
    
    if not os.path.isdir(full_path):
        return {"success": False, "error": f"Not a directory: {path}"}
    
    files = []
    try:
        items = os.listdir(full_path)
        for item in items:
            # è·³è¿‡éšè—æ–‡ä»¶
            if not show_hidden and item.startswith('.'):
                continue
            
            item_path = os.path.join(full_path, item)
            stat = os.stat(item_path)
            
            files.append({
                "name": item,
                "type": "dir" if os.path.isdir(item_path) else "file",
                "size": stat.st_size if os.path.isfile(item_path) else None,
                "modified": stat.st_mtime,
                "path": os.path.relpath(item_path, sandbox_path)
            })
        
        # æ’åºï¼šç›®å½•åœ¨å‰ï¼Œæ–‡ä»¶åœ¨åï¼ŒæŒ‰åç§°æ’åº
        files.sort(key=lambda x: (0 if x["type"] == "dir" else 1, x["name"].lower()))
        
    except PermissionError:
        return {"success": False, "error": f"Permission denied: {path}"}
    
    return {
        "success": True,
        "data": files,
        "path": path,
        "total": len(files)
    }


def _read_file(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    # æ”¯æŒå¤šç§å‚æ•°åï¼šfile_path æˆ– path
    file_path = params.get('file_path') or params.get('path')
    encoding = params.get('encoding', 'utf-8')
    limit = params.get('limit', 1000)  # é»˜è®¤æœ€å¤šè¯»å–1000è¡Œ
    
    if not file_path:
        return {"success": False, "error": "Missing required parameter: file_path (or path)"}
    
    full_path = _validate_path(sandbox_path, file_path)
    
    if not os.path.exists(full_path):
        return {"success": False, "error": f"File not found: {file_path}"}
    
    if os.path.isdir(full_path):
        return {"success": False, "error": f"Is a directory: {file_path}"}
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(full_path)
    if file_size > MAX_FILE_SIZE:
        return {
            "success": False, 
            "error": f"File too large ({file_size} bytes), max: {MAX_FILE_SIZE} bytes"
        }
    
    try:
        with open(full_path, 'r', encoding=encoding, errors='replace') as f:
            if limit:
                lines = []
                for i, line in enumerate(f):
                    if i >= limit:
                        lines.append(f"\n... ({limit} lines shown, file truncated)")
                        break
                    lines.append(line)
                content = ''.join(lines)
            else:
                content = f.read()
        
        return {
            "success": True,
            "data": content,
            "file_path": file_path,
            "size": file_size,
            "lines": content.count('\n') + 1
        }
        
    except UnicodeDecodeError:
        # äºŒè¿›åˆ¶æ–‡ä»¶å°è¯•ä»¥ base64 è¿”å›
        import base64
        with open(full_path, 'rb') as f:
            data = f.read()
        return {
            "success": True,
            "data": base64.b64encode(data).decode('utf-8'),
            "encoding": "base64",
            "file_path": file_path,
            "size": file_size
        }
    except Exception as e:
        return {"success": False, "error": f"Read failed: {str(e)}"}


def _write_file(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """å†™å…¥æ–‡ä»¶"""
    # æ”¯æŒå¤šç§å‚æ•°åï¼šfile_path æˆ– path
    file_path = params.get('file_path') or params.get('path')
    content = params.get('content')
    encoding = params.get('encoding', 'utf-8')
    append = params.get('append', False)
    
    if not file_path:
        return {"success": False, "error": "Missing required parameter: file_path (or path)"}
    
    if content is None:
        return {"success": False, "error": "Missing required parameter: content"}
    
    full_path = _validate_path(sandbox_path, file_path)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    dir_path = os.path.dirname(full_path)
    if dir_path:
        os.makedirs(dir_path, exist_ok=True)
    
    try:
        mode = 'a' if append else 'w'
        with open(full_path, mode, encoding=encoding) as f:
            f.write(content)
        
        file_size = os.path.getsize(full_path)
        
        return {
            "success": True,
            "data": f"{'Appended to' if append else 'Written to'} {file_path}",
            "file_path": file_path,
            "size": file_size,
            "bytes_written": len(content.encode(encoding))
        }
        
    except Exception as e:
        return {"success": False, "error": f"Write failed: {str(e)}"}


def _delete_file(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ é™¤æ–‡ä»¶æˆ–ç›®å½•"""
    # æ”¯æŒå¤šç§å‚æ•°åï¼šfile_path æˆ– path
    file_path = params.get('file_path') or params.get('path')
    recursive = params.get('recursive', False)
    
    if not file_path:
        return {"success": False, "error": "Missing required parameter: file_path (or path)"}
    
    full_path = _validate_path(sandbox_path, file_path)
    
    if not os.path.exists(full_path):
        return {"success": False, "error": f"Path not found: {file_path}"}
    
    try:
        if os.path.isdir(full_path):
            if recursive:
                shutil.rmtree(full_path)
                return {"success": True, "data": f"Directory deleted recursively: {file_path}"}
            else:
                os.rmdir(full_path)
                return {"success": True, "data": f"Directory deleted: {file_path}"}
        else:
            os.remove(full_path)
            return {"success": True, "data": f"File deleted: {file_path}"}
            
    except OSError as e:
        return {"success": False, "error": f"Delete failed: {str(e)}"}


def _make_directory(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ›å»ºç›®å½•"""
    dir_path = params.get('dir_path')
    parents = params.get('parents', True)
    
    if not dir_path:
        return {"success": False, "error": "Missing required parameter: dir_path"}
    
    full_path = _validate_path(sandbox_path, dir_path)
    
    try:
        if parents:
            os.makedirs(full_path, exist_ok=True)
        else:
            os.mkdir(full_path)
        
        return {"success": True, "data": f"Directory created: {dir_path}"}
        
    except FileExistsError:
        return {"success": False, "error": f"Directory already exists: {dir_path}"}
    except Exception as e:
        return {"success": False, "error": f"Create directory failed: {str(e)}"}


def _copy_file(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """å¤åˆ¶æ–‡ä»¶æˆ–ç›®å½•"""
    src = params.get('src')
    dst = params.get('dst')
    
    if not src or not dst:
        return {"success": False, "error": "Missing required parameters: src and dst"}
    
    full_src = _validate_path(sandbox_path, src)
    full_dst = _validate_path(sandbox_path, dst)
    
    if not os.path.exists(full_src):
        return {"success": False, "error": f"Source not found: {src}"}
    
    try:
        if os.path.isdir(full_src):
            shutil.copytree(full_src, full_dst)
        else:
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            dst_dir = os.path.dirname(full_dst)
            if dst_dir:
                os.makedirs(dst_dir, exist_ok=True)
            
            # å…ˆå°è¯•ä½¿ç”¨ copy2ï¼ˆå¤åˆ¶å†…å®¹å’Œå…ƒæ•°æ®ï¼‰
            try:
                shutil.copy2(full_src, full_dst)
            except (PermissionError, OSError) as e:
                # å¦‚æœ copy2 å¤±è´¥ï¼ˆé€šå¸¸æ˜¯å…ƒæ•°æ®é—®é¢˜ï¼‰ï¼Œå°è¯•ä½¿ç”¨ copyï¼ˆä»…å¤åˆ¶å†…å®¹ï¼‰
                # å¹¶æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»æˆåŠŸå¤åˆ¶
                if os.path.exists(full_dst) and os.path.getsize(full_dst) == os.path.getsize(full_src):
                    # æ–‡ä»¶å·²æˆåŠŸå¤åˆ¶ï¼Œåªæ˜¯å…ƒæ•°æ®ï¼ˆæƒé™/æ—¶é—´æˆ³ï¼‰è®¾ç½®å¤±è´¥
                    return {
                        "success": True, 
                        "data": f"Copied {src} to {dst} (content copied, metadata warning: {str(e)})"
                    }
                else:
                    # æ–‡ä»¶æœªæˆåŠŸå¤åˆ¶ï¼Œå°è¯•åŸºæœ¬çš„ copy
                    shutil.copy(full_src, full_dst)
        
        # éªŒè¯å¤åˆ¶æ˜¯å¦æˆåŠŸ
        if os.path.exists(full_dst):
            src_size = os.path.getsize(full_src)
            dst_size = os.path.getsize(full_dst)
            if src_size == dst_size:
                return {"success": True, "data": f"Copied {src} to {dst} ({dst_size} bytes)"}
            else:
                return {"success": False, "error": f"Copy incomplete: source {src_size} bytes, destination {dst_size} bytes"}
        else:
            return {"success": False, "error": f"Copy failed: destination file not created"}
        
    except PermissionError as e:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å®é™…ä¸Šå·²è¢«å¤åˆ¶ï¼ˆæŸäº›æƒ…å†µä¸‹æƒé™é”™è¯¯å‘ç”Ÿåœ¨å…ƒæ•°æ®ä¿®æ”¹é˜¶æ®µï¼‰
        if os.path.exists(full_dst):
            return {
                "success": True, 
                "data": f"Copied {src} to {dst} (with permission warning: {str(e)})"
            }
        return {"success": False, "error": f"Permission denied: {str(e)}"}
    except Exception as e:
        # æœ€åçš„æ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶å­˜åœ¨ä¸”å¤§å°ç›¸åŒï¼Œè®¤ä¸ºå¤åˆ¶æˆåŠŸ
        if os.path.exists(full_dst) and os.path.exists(full_src):
            try:
                if os.path.getsize(full_dst) == os.path.getsize(full_src):
                    return {
                        "success": True, 
                        "data": f"Copied {src} to {dst} (with warning: {str(e)})"
                    }
            except:
                pass
        return {"success": False, "error": f"Copy failed: {str(e)}"}


def _move_file(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """ç§»åŠ¨/é‡å‘½åæ–‡ä»¶æˆ–ç›®å½•

    æ”¯æŒä¸¤ç§å‚æ•°åï¼š
    - src/dst (æ¨è)
    - source/destination (å…¼å®¹ Kotlin ä»£ç )
    """
    # æ”¯æŒä¸¤ç§å‚æ•°å
    src = params.get('src') or params.get('source')
    dst = params.get('dst') or params.get('destination')

    if not src or not dst:
        return {"success": False, "error": "Missing required parameters: src/source and dst/destination"}
    
    full_src = _validate_path(sandbox_path, src)
    full_dst = _validate_path(sandbox_path, dst)
    
    if not os.path.exists(full_src):
        return {"success": False, "error": f"Source not found: {src}"}
    
    try:
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        dst_dir = os.path.dirname(full_dst)
        if dst_dir:
            os.makedirs(dst_dir, exist_ok=True)
        
        shutil.move(full_src, full_dst)
        
        return {"success": True, "data": f"Moved {src} to {dst}"}
        
    except Exception as e:
        return {"success": False, "error": f"Move failed: {str(e)}"}


def _file_stat(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """è·å–æ–‡ä»¶/ç›®å½•ä¿¡æ¯"""
    # æ”¯æŒå¤šç§å‚æ•°åï¼šfile_path æˆ– path
    file_path = params.get('file_path') or params.get('path')
    
    if not file_path:
        return {"success": False, "error": "Missing required parameter: file_path (or path)"}
    
    full_path = _validate_path(sandbox_path, file_path)
    
    if not os.path.exists(full_path):
        return {"success": False, "error": f"Path not found: {file_path}"}
    
    try:
        stat = os.stat(full_path)
        
        return {
            "success": True,
            "data": {
                "name": os.path.basename(file_path),
                "path": file_path,
                "type": "dir" if os.path.isdir(full_path) else "file",
                "size": stat.st_size,
                "created": stat.st_ctime,
                "modified": stat.st_mtime,
                "accessed": stat.st_atime,
                "permissions": oct(stat.st_mode)[-3:],
            }
        }
        
    except Exception as e:
        return {"success": False, "error": f"Stat failed: {str(e)}"}


def _file_exists(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """æ£€æŸ¥æ–‡ä»¶/ç›®å½•æ˜¯å¦å­˜åœ¨"""
    # æ”¯æŒå¤šç§å‚æ•°åï¼šfile_path æˆ– path
    file_path = params.get('file_path') or params.get('path')
    
    if not file_path:
        return {"success": False, "error": "Missing required parameter: file_path (or path)"}
    
    full_path = _validate_path(sandbox_path, file_path)
    
    exists = os.path.exists(full_path)
    is_file = os.path.isfile(full_path) if exists else False
    is_dir = os.path.isdir(full_path) if exists else False
    
    return {
        "success": True,
        "data": {
            "exists": exists,
            "is_file": is_file,
            "is_directory": is_dir,
            "path": file_path
        }
    }


def _list_available_packages(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """åˆ—å‡ºæ²™ç®±ç¯å¢ƒä¸­å¯ç”¨çš„ Python åŒ…ï¼ˆåŸºäºå·²æˆåŠŸå¯¼å…¥çš„æ¨¡å—ï¼‰"""
    global _PREIMPORTED_MODULES
    
    # åˆå§‹åŒ–é¢„å¯¼å…¥æ¨¡å—
    if not _PREIMPORTED_MODULES:
        _PREIMPORTED_MODULES = _init_preimported_modules()
    
    # æå–åŒ…åï¼ˆç§»é™¤åˆ«åï¼Œåªä¿ç•™ä¸»æ¨¡å—åï¼‰
    packages = []
    for name, module in _PREIMPORTED_MODULES.items():
        # è¿‡æ»¤æ‰åˆ«åï¼ˆå¦‚ 'np' æ˜¯ numpy çš„åˆ«åï¼‰
        if name not in ['np', 'pd']:
            packages.append(name)
    
    # æ ¼å¼åŒ–è¾“å‡º
    package_list = '\n'.join(sorted(packages))
    
    return {
        "success": True,
        "packages": package_list,
        "package_count": len(packages),
        "message": f"Found {len(packages)} available packages in Chaquopy sandbox environment",
        "source": "chaquopy_pre_imported"
    }


# å…¼å®¹æ—§æ¥å£
run = execute


# é¢„å¯¼å…¥çš„å¸¸ç”¨æ¨¡å—ï¼ŒAI æ‰§è¡Œä»£ç æ—¶å¯ç›´æ¥ä½¿ç”¨
_PREIMPORTED_MODULES = {}

def _init_preimported_modules():
    """åˆå§‹åŒ–é¢„å¯¼å…¥çš„æ¨¡å—å­—å…¸"""
    modules = {}
    
    # æ ‡å‡†åº“
    try:
        import json as _json
        modules['json'] = _json
    except: pass
    
    try:
        import csv as _csv
        modules['csv'] = _csv
    except: pass
    
    try:
        import re as _re
        modules['re'] = _re
    except: pass
    
    try:
        import math as _math
        modules['math'] = _math
    except: pass
    
    try:
        import random as _random
        modules['random'] = _random
    except: pass
    
    try:
        import datetime as _datetime
        modules['datetime'] = _datetime
    except: pass
    
    try:
        from dateutil import parser as _dateutil_parser
        modules['dateutil_parser'] = _dateutil_parser
    except: pass
    
    try:
        import itertools as _itertools
        modules['itertools'] = _itertools
    except: pass
    
    try:
        import collections as _collections
        modules['collections'] = _collections
    except: pass
    
    try:
        import hashlib as _hashlib
        modules['hashlib'] = _hashlib
    except: pass
    
    try:
        import base64 as _base64
        modules['base64'] = _base64
    except: pass
    
    try:
        import io as _io
        modules['io'] = _io
    except: pass
    
    try:
        import pathlib as _pathlib
        modules['pathlib'] = _pathlib
    except: pass
    
    try:
        import sqlite3 as _sqlite3
        modules['sqlite3'] = _sqlite3
    except: pass
    
    # æ•°æ®å¤„ç†
    try:
        import numpy as _np
        modules['np'] = modules['numpy'] = _np
    except: pass
    
    try:
        import pandas as _pd
        modules['pd'] = modules['pandas'] = _pd
    except: pass
    
    # å›¾åƒå¤„ç†
    try:
        from PIL import Image as _Image, ImageOps as _ImageOps, ImageFilter as _ImageFilter
        modules['Image'] = _Image
        modules['ImageOps'] = _ImageOps
        modules['ImageFilter'] = _ImageFilter
    except: pass
    
    # ç½‘ç»œè¯·æ±‚
    try:
        import requests as _requests
        modules['requests'] = _requests
    except: pass
    
    try:
        from bs4 import BeautifulSoup as _BeautifulSoup
        modules['BeautifulSoup'] = _BeautifulSoup
    except: pass
    
    # PDFå¤„ç†
    try:
        from PyPDF2 import PdfReader as _PdfReader, PdfWriter as _PdfWriter
        modules['PdfReader'] = _PdfReader
        modules['PdfWriter'] = _PdfWriter
    except: pass
    
    # YAML/TOML
    try:
        import yaml as _yaml
        modules['yaml'] = _yaml
    except: pass
    
    try:
        import toml as _toml
        modules['toml'] = _toml
    except: pass
    
    return modules


def _python_exec(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œ Python ä»£ç ï¼ˆä½¿ç”¨ Chaquopy ç¯å¢ƒï¼‰
    è¿™æ˜¯ç»•è¿‡ Android shell é™åˆ¶çš„ä¸»è¦æ–¹å¼
    
    é¢„å¯¼å…¥çš„æ¨¡å—å¯ç›´æ¥ä½¿ç”¨:
    - json, csv, re, math, random, datetime, hashlib, base64
    - np (numpy), pd (pandas), Image (PIL)
    - requests, BeautifulSoup (bs4)
    - PdfReader, PdfWriter (PyPDF2)
    - yaml, toml
    
    è¾…åŠ©å‡½æ•°:
    - read_file(path): è¯»å–æ–‡æœ¬æ–‡ä»¶
    - write_file(path, content): å†™å…¥æ–‡æœ¬æ–‡ä»¶
    - list_files(path='.'): åˆ—å‡ºç›®å½•æ–‡ä»¶
    - download(url, save_path=None): ä¸‹è½½æ–‡ä»¶
    """
    global _PREIMPORTED_MODULES
    
    code = params.get('code')
    script_path = params.get('script_path')  # å¯é€‰ï¼šä»æ–‡ä»¶è¯»å–ä»£ç 
    
    if not code and not script_path:
        return {"success": False, "error": "Missing required parameter: code or script_path"}
    
    if script_path:
        full_script_path = _validate_path(sandbox_path, script_path)
        if not os.path.exists(full_script_path):
            return {"success": False, "error": f"Script file not found: {script_path}"}
        try:
            with open(full_script_path, 'r', encoding='utf-8') as f:
                code = f.read()
        except Exception as e:
            return {"success": False, "error": f"Failed to read script: {str(e)}"}
    
    # å®‰å…¨æ£€æŸ¥ï¼šç¦æ­¢çš„å±é™©æ“ä½œ
    dangerous_patterns = ['os.system', 'subprocess', 'eval(', 'exec(', '__import__']
    for dangerous in dangerous_patterns:
        if dangerous in code:
            return {"success": False, "error": f"Security check failed: forbidden pattern '{dangerous}'"}
    
    # åˆå§‹åŒ–é¢„å¯¼å…¥æ¨¡å—
    if not _PREIMPORTED_MODULES:
        _PREIMPORTED_MODULES = _init_preimported_modules()
    
    # åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
    import io
    import sys
    import traceback
    
    # ä¿å­˜åŸå§‹ stdout/stderr
    old_stdout = sys.stdout
    old_stderr = sys.stderr
    
    # åˆ›å»ºæ–°çš„è¾“å‡ºæ•è·
    stdout_capture = io.StringIO()
    stderr_capture = io.StringIO()
    
    # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨è¯»å–æ–‡ä»¶
    def _safe_read_file(path: str, encoding: str = 'utf-8') -> str:
        full_path = _validate_path(sandbox_path, path)
        with open(full_path, 'r', encoding=encoding, errors='replace') as f:
            return f.read()
    
    # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨å†™å…¥æ–‡ä»¶
    def _safe_write_file(path: str, content: str, encoding: str = 'utf-8'):
        full_path = _validate_path(sandbox_path, path)
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        with open(full_path, 'w', encoding=encoding) as f:
            f.write(content)
    
    # è¾…åŠ©å‡½æ•°ï¼šåˆ—å‡ºæ–‡ä»¶
    def _safe_list_files(path: str = '.') -> List[str]:
        full_path = _validate_path(sandbox_path, path)
        return os.listdir(full_path)
    
    # è¾…åŠ©å‡½æ•°ï¼šä¸‹è½½æ–‡ä»¶
    def _safe_download(url: str, save_path: str = None) -> str:
        if 'requests' not in _PREIMPORTED_MODULES:
            raise ImportError("requests module not available")
        resp = _PREIMPORTED_MODULES['requests'].get(url, timeout=30)
        resp.raise_for_status()
        if save_path:
            full_path = _validate_path(sandbox_path, save_path)
            with open(full_path, 'wb') as f:
                f.write(resp.content)
            return save_path
        return resp.text
    
    # å‡†å¤‡æ‰§è¡Œç¯å¢ƒ - åŒ…å«é¢„å¯¼å…¥æ¨¡å—
    exec_globals = {
        '__builtins__': __builtins__,
        '__name__': '__main__',
        '__file__': os.path.join(sandbox_path, 'script.py'),
        # é¢„å¯¼å…¥æ¨¡å—
        **_PREIMPORTED_MODULES,
        # è¾…åŠ©å‡½æ•°
        'read_file': _safe_read_file,
        'write_file': _safe_write_file,
        'list_files': _safe_list_files,
        'download': _safe_download,
    }
    exec_locals = {}
    
    # ä¿®æ”¹å·¥ä½œç›®å½•
    original_cwd = os.getcwd()
    os.chdir(sandbox_path)
    
    # æ·»åŠ æ²™ç®±çš„ site-packages åˆ° Python è·¯å¾„ï¼Œä»¥ä¾¿ import ç”¨æˆ·å®‰è£…çš„åŒ…
    site_packages = os.path.join(sandbox_path, 'lib', 'python3.11', 'site-packages')
    if os.path.exists(site_packages) and site_packages not in sys.path:
        sys.path.insert(0, site_packages)
    
    try:
        # é‡å®šå‘è¾“å‡º
        sys.stdout = stdout_capture
        sys.stderr = stderr_capture
        
        # æ‰§è¡Œä»£ç 
        exec(code, exec_globals, exec_locals)
        
        # æ¢å¤è¾“å‡º
        sys.stdout = old_stdout
        sys.stderr = old_stderr
        
        # è·å–è¾“å‡º
        stdout_output = stdout_capture.getvalue()
        stderr_output = stderr_capture.getvalue()
        
        # æ¢å¤å·¥ä½œç›®å½•
        os.chdir(original_cwd)
        
        # ä¿®å¤ï¼šç¡®ä¿stdoutä¸ä¸ºNoneï¼Œä¸”å¤„ç†ç©ºè¾“å‡ºæƒ…å†µ
        result = {
            "success": True,
            "stdout": stdout_output if stdout_output else "",
            "stderr": stderr_output if stderr_output else None,
        }
        
        # å¦‚æœä»£ç ä¸­å®šä¹‰äº† result å˜é‡ï¼ŒåŒ…å«å®ƒ
        if 'result' in exec_locals:
            try:
                result['result'] = _to_json_serializable(exec_locals['result'])
            except:
                result['result'] = str(exec_locals['result'])
        
        return result
        
    except Exception as e:
        # æ¢å¤è¾“å‡ºï¼ˆç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½æ¢å¤ï¼‰
        sys.stdout = old_stdout
        sys.stderr = old_stderr
        os.chdir(original_cwd)
        
        # è·å–é”™è¯¯ä¿¡æ¯
        exc_type, exc_value, exc_traceback = sys.exc_info()
        error_msg = ''.join(traceback.format_exception(exc_type, exc_value, exc_traceback))
        
        # ä¿®å¤ï¼šç¡®ä¿è¿”å›å®Œæ•´çš„é”™è¯¯ä¿¡æ¯
        return {
            "success": False,
            "error": f"Python execution failed: {str(e)}",
            "stdout": stdout_capture.getvalue(),
            "stderr": stderr_capture.getvalue(),
            "traceback": error_msg
        }


# =============================================================================
# ä¾¿åˆ©æ“ä½œ - å¸¸ç”¨åŠŸèƒ½çš„å°è£…
# =============================================================================

def _process_image(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    å›¾ç‰‡å¤„ç†ä¾¿åˆ©æ“ä½œ
    
    Params:
        - input_path: è¾“å…¥å›¾ç‰‡è·¯å¾„
        - output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤è¦†ç›–åŸæ–‡ä»¶æˆ–åŠ åç¼€ï¼‰
        - operation: æ“ä½œç±»å‹ - resize, convert, compress, thumbnail, grayscale
        - width: ç›®æ ‡å®½åº¦ï¼ˆresize/thumbnailï¼‰
        - height: ç›®æ ‡é«˜åº¦ï¼ˆresize/thumbnailï¼Œå¯é€‰ï¼‰
        - format: ç›®æ ‡æ ¼å¼ï¼ˆconvertï¼‰- JPEG, PNG, WEBP
        - quality: å‹ç¼©è´¨é‡ 1-95ï¼ˆcompressï¼Œé»˜è®¤ 85ï¼‰
    """
    try:
        from PIL import Image, ImageOps
    except ImportError:
        return {"success": False, "error": "PIL (Pillow) not available"}
    
    input_path = params.get('input_path')
    output_path = params.get('output_path')
    operation = params.get('operation', 'resize')
    
    if not input_path:
        return {"success": False, "error": "Missing input_path parameter"}
    
    try:
        full_input = _validate_path(sandbox_path, input_path)
        if not os.path.exists(full_input):
            return {"success": False, "error": f"Input file not found: {input_path}"}
        
        # æ‰“å¼€å›¾ç‰‡
        img = Image.open(full_input)
        original_size = img.size
        original_format = img.format
        
        # æ‰§è¡Œæ“ä½œ
        if operation == 'resize':
            width = params.get('width')
            height = params.get('height')
            if not width:
                return {"success": False, "error": "Resize requires width parameter"}
            if height:
                img = img.resize((width, height), Image.Resampling.LANCZOS)
            else:
                # ä¿æŒå®½é«˜æ¯”
                ratio = width / original_size[0]
                height = int(original_size[1] * ratio)
                img = img.resize((width, height), Image.Resampling.LANCZOS)
        
        elif operation == 'thumbnail':
            max_size = params.get('width', 800)
            img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
        
        elif operation == 'grayscale':
            img = ImageOps.grayscale(img)
        
        elif operation == 'convert':
            fmt = params.get('format', 'JPEG').upper()
            if img.mode in ('RGBA', 'P') and fmt == 'JPEG':
                img = img.convert('RGB')
        
        elif operation == 'compress':
            quality = params.get('quality', 85)
            # å‹ç¼©é€šè¿‡ä¿å­˜æ—¶è®¾ç½®è´¨é‡å®ç°
            pass
        
        else:
            return {"success": False, "error": f"Unknown operation: {operation}"}
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if not output_path:
            name, ext = os.path.splitext(input_path)
            if operation == 'convert':
                fmt = params.get('format', 'JPEG').lower()
                output_path = f"{name}_converted.{fmt}"
            else:
                output_path = f"{name}_{operation}{ext}"
        
        full_output = _validate_path(sandbox_path, output_path)
        os.makedirs(os.path.dirname(full_output), exist_ok=True)
        
        # ä¿å­˜
        save_kwargs = {}
        if operation == 'compress' or original_format in ('JPEG', None):
            save_kwargs['quality'] = params.get('quality', 85)
            save_kwargs['optimize'] = True
        
        img.save(full_output, **save_kwargs)
        
        final_size = os.path.getsize(full_output)
        
        return {
            "success": True,
            "data": f"Image {operation} completed",
            "input_path": input_path,
            "output_path": output_path,
            "original_size": original_size,
            "new_size": img.size,
            "file_size_bytes": final_size,
            "file_size_kb": round(final_size / 1024, 2)
        }
        
    except Exception as e:
        return {"success": False, "error": f"Image processing failed: {str(e)}"}


def _convert_excel(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Excel è½¬æ¢ä¾¿åˆ©æ“ä½œ
    
    Params:
        - input_path: è¾“å…¥ Excel è·¯å¾„
        - output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤åŒå.csvæˆ–.jsonï¼‰
        - format: è¾“å‡ºæ ¼å¼ - csv, json, json_recordsï¼ˆé»˜è®¤ csvï¼‰
        - sheet: å·¥ä½œè¡¨åç§°æˆ–ç´¢å¼•ï¼ˆé»˜è®¤ 0ï¼‰
        - preview_only: ä»…é¢„è§ˆå‰Nè¡Œï¼Œä¸ä¿å­˜ï¼ˆé»˜è®¤ Falseï¼‰
        - preview_rows: é¢„è§ˆè¡Œæ•°ï¼ˆé»˜è®¤ 10ï¼‰
    """
    try:
        import pandas as pd
    except ImportError:
        return {"success": False, "error": "pandas not available"}
    
    input_path = params.get('input_path')
    output_path = params.get('output_path')
    fmt = params.get('format', 'csv').lower()
    sheet = params.get('sheet', 0)
    preview_only = params.get('preview_only', False)
    preview_rows = params.get('preview_rows', 10)
    
    if not input_path:
        return {"success": False, "error": "Missing input_path parameter"}
    
    try:
        full_input = _validate_path(sandbox_path, input_path)
        if not os.path.exists(full_input):
            return {"success": False, "error": f"Input file not found: {input_path}"}
        
        # è¯»å– Excel
        df = pd.read_excel(full_input, sheet_name=sheet)
        
        row_count = len(df)
        col_count = len(df.columns)
        columns = list(df.columns)
        
        if preview_only:
            preview = df.head(preview_rows).to_dict(orient='records')
            return {
                "success": True,
                "preview": preview,
                "total_rows": row_count,
                "total_columns": col_count,
                "columns": columns
            }
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if not output_path:
            name, _ = os.path.splitext(input_path)
            output_path = f"{name}.{fmt}"
        
        full_output = _validate_path(sandbox_path, output_path)
        os.makedirs(os.path.dirname(full_output), exist_ok=True)
        
        # è½¬æ¢å¹¶ä¿å­˜
        if fmt == 'csv':
            df.to_csv(full_output, index=False, encoding='utf-8-sig')
        elif fmt in ('json', 'json_records'):
            df.to_json(full_output, orient='records', force_ascii=False, indent=2)
        elif fmt == 'json_lines':
            df.to_json(full_output, orient='records', lines=True, force_ascii=False)
        else:
            return {"success": False, "error": f"Unsupported format: {fmt}"}
        
        return {
            "success": True,
            "data": f"Converted to {fmt}",
            "input_path": input_path,
            "output_path": output_path,
            "rows": row_count,
            "columns": col_count,
            "column_names": columns
        }
        
    except Exception as e:
        return {"success": False, "error": f"Excel conversion failed: {str(e)}"}


def _extract_pdf_text(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    PDF æ–‡æœ¬æå–ä¾¿åˆ©æ“ä½œ
    
    Params:
        - input_path: PDF æ–‡ä»¶è·¯å¾„
        - pages: é¡µç åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤å…¨éƒ¨ï¼‰
        - output_path: è¾“å‡ºæ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        - max_chars: è¿”å›çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤å…¨éƒ¨ï¼‰
    """
    try:
        from PyPDF2 import PdfReader
    except ImportError:
        return {"success": False, "error": "PyPDF2 not available"}
    
    input_path = params.get('input_path')
    pages = params.get('pages')  # None = all pages
    output_path = params.get('output_path')
    max_chars = params.get('max_chars')
    
    if not input_path:
        return {"success": False, "error": "Missing input_path parameter"}
    
    try:
        full_input = _validate_path(sandbox_path, input_path)
        if not os.path.exists(full_input):
            return {"success": False, "error": f"Input file not found: {input_path}"}
        
        reader = PdfReader(full_input)
        total_pages = len(reader.pages)
        
        # ç¡®å®šè¦å¤„ç†çš„é¡µé¢
        if pages is None:
            pages = list(range(total_pages))
        else:
            # è½¬æ¢ä¸º0-basedç´¢å¼•
            pages = [p - 1 if p > 0 else p for p in pages]
            pages = [p for p in pages if 0 <= p < total_pages]
        
        # æå–æ–‡æœ¬
        extracted = []
        for page_num in pages:
            page = reader.pages[page_num]
            text = page.extract_text() or ""
            extracted.append({
                'page': page_num + 1,
                'text': text
            })
        
        full_text = "\\n\\n".join(item['text'] for item in extracted)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_path:
            full_output = _validate_path(sandbox_path, output_path)
            with open(full_output, 'w', encoding='utf-8') as f:
                f.write(full_text)
        
        # æˆªæ–­è¿”å›çš„æ–‡æœ¬
        return_text = full_text
        truncated = False
        if max_chars and len(full_text) > max_chars:
            return_text = full_text[:max_chars] + "\\n...[truncated]"
            truncated = True
        
        return {
            "success": True,
            "data": return_text,
            "total_pages": total_pages,
            "extracted_pages": len(extracted),
            "total_chars": len(full_text),
            "truncated": truncated,
            "output_file": output_path if output_path else None
        }
        
    except Exception as e:
        return {"success": False, "error": f"PDF extraction failed: {str(e)}"}


def _download_file(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ–‡ä»¶ä¸‹è½½ä¾¿åˆ©æ“ä½œ
    
    Params:
        - url: ä¸‹è½½é“¾æ¥
        - output_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»URLæå–æ–‡ä»¶åï¼‰
        - timeout: è¶…æ—¶ç§’æ•°ï¼ˆé»˜è®¤ 60ï¼‰
        - headers: è‡ªå®šä¹‰è¯·æ±‚å¤´ï¼ˆå¯é€‰ï¼‰
    """
    try:
        import requests
    except ImportError:
        return {"success": False, "error": "requests not available"}
    
    url = params.get('url')
    output_path = params.get('output_path')
    timeout = params.get('timeout', 60)
    headers = params.get('headers', {'User-Agent': 'Mozilla/5.0 (compatible; Bot/1.0)'})
    
    if not url:
        return {"success": False, "error": "Missing url parameter"}
    
    try:
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
        if not output_path:
            # ä»URLæå–æ–‡ä»¶å
            from urllib.parse import urlparse
            parsed = urlparse(url)
            output_path = os.path.basename(parsed.path) or 'downloaded_file'
        
        full_output = _validate_path(sandbox_path, output_path)
        os.makedirs(os.path.dirname(full_output), exist_ok=True)
        
        # ä¸‹è½½
        response = requests.get(url, headers=headers, timeout=timeout, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(full_output, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
        
        return {
            "success": True,
            "data": f"Downloaded successfully",
            "url": url,
            "output_path": output_path,
            "file_size_bytes": downloaded,
            "file_size_kb": round(downloaded / 1024, 2),
            "content_type": response.headers.get('content-type')
        }
        
    except Exception as e:
        return {"success": False, "error": f"Download failed: {str(e)}"}


def _sqlite_query(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œ SQLite æŸ¥è¯¢
    
    Params:
        - db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æ²™ç®±ï¼‰
        - query: SQL æŸ¥è¯¢è¯­å¥
        - params: æŸ¥è¯¢å‚æ•°ï¼ˆå¯é€‰ï¼Œç”¨äºå‚æ•°åŒ–æŸ¥è¯¢ï¼‰
        - max_rows: æœ€å¤§è¿”å›è¡Œæ•°ï¼ˆé»˜è®¤ 1000ï¼‰
    """
    import sqlite3
    
    db_path = params.get('db_path')
    query = params.get('query')
    query_params = params.get('params', ())
    max_rows = params.get('max_rows', 1000)
    
    if not db_path:
        return {"success": False, "error": "Missing db_path parameter"}
    if not query:
        return {"success": False, "error": "Missing query parameter"}
    
    try:
        full_db_path = _validate_path(sandbox_path, db_path)
        if not os.path.exists(full_db_path):
            return {"success": False, "error": f"Database not found: {db_path}"}
        
        conn = sqlite3.connect(full_db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # æ‰§è¡ŒæŸ¥è¯¢
        cursor.execute(query, query_params)
        
        # åˆ¤æ–­æ˜¯å¦ä¸º SELECT æŸ¥è¯¢
        if query.strip().upper().startswith('SELECT') or query.strip().upper().startswith('PRAGMA'):
            rows = cursor.fetchmany(max_rows)
            columns = [description[0] for description in cursor.description] if cursor.description else []
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            results = []
            for row in rows:
                results.append({key: row[key] for key in row.keys()})
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šè¡Œï¼ˆåœ¨å…³é—­è¿æ¥å‰ï¼‰
            has_more = cursor.fetchone() is not None
            
            conn.close()
            
            # ç”Ÿæˆäººç±»å¯è¯»çš„è¾“å‡º
            preview = []
            for i, row in enumerate(results[:3]):
                row_str = ", ".join([f"{k}={v}" for k, v in row.items()])
                preview.append(f"  è¡Œ{i+1}: {row_str}")
            if len(results) > 3:
                preview.append(f"  ... è¿˜æœ‰ {len(results) - 3} è¡Œ")
            
            return {
                "success": True,
                "data": results,
                "stdout": f"ğŸ—„ï¸ SQLite æŸ¥è¯¢\nğŸ“Š è¿”å› {len(results)} è¡Œ\nğŸ“‹ åˆ—: {', '.join(columns)}\n\nğŸ“„ é¢„è§ˆ:\n" + "\n".join(preview) if preview else "(æ— æ•°æ®)",
                "columns": columns,
                "row_count": len(results),
                "truncated": has_more
            }
        else:
            # INSERT/UPDATE/DELETE/CREATE ç­‰
            conn.commit()
            affected = cursor.rowcount
            conn.close()
            
            return {
                "success": True,
                "data": f"Query executed successfully",
                "stdout": f"ğŸ—„ï¸ SQL æ‰§è¡ŒæˆåŠŸ\nâœï¸ å½±å“è¡Œæ•°: {affected}",
                "rows_affected": affected
            }
        
    except sqlite3.Error as e:
        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        error_msg = f"SQLite error: {str(e)}"
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§çš„è¯­æ³•é”™è¯¯
        query_stripped = query.strip() if query else ""
        if "LIMIT" in str(e) and "syntax error" in str(e):
            error_msg = f"SQLite è¯­æ³•é”™è¯¯ (LIMIT): è¯·æ£€æŸ¥ LIMIT å…³é”®è¯å‰æ˜¯å¦æœ‰ç©ºæ ¼\nç¤ºä¾‹: SELECT * FROM table LIMIT 10\n\nåŸå§‹é”™è¯¯: {str(e)}"
        elif "syntax error" in str(e):
            # æ‰¾å‡ºé”™è¯¯ä½ç½®é™„è¿‘çš„å†…å®¹
            error_msg = f"SQL è¯­æ³•é”™è¯¯: {str(e)}\n\næŸ¥è¯¢è¯­å¥:\n{query_stripped[:200]}"
        return {"success": False, "error": error_msg}
    except Exception as e:
        return {"success": False, "error": f"Query failed: {str(e)}\n\næŸ¥è¯¢è¯­å¥:\n{query_stripped[:200] if query else '(empty)'}"}


def _sqlite_tables(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    è·å– SQLite æ•°æ®åº“çš„è¡¨ç»“æ„ä¿¡æ¯
    
    Params:
        - db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        - detail: æ˜¯å¦åŒ…å«åˆ—è¯¦æƒ…ï¼ˆé»˜è®¤ Trueï¼‰
    """
    import sqlite3
    
    db_path = params.get('db_path')
    detail = params.get('detail', True)
    
    if not db_path:
        return {"success": False, "error": "Missing db_path parameter"}
    
    try:
        full_db_path = _validate_path(sandbox_path, db_path)
        if not os.path.exists(full_db_path):
            return {"success": False, "error": f"Database not found: {db_path}"}
        
        conn = sqlite3.connect(full_db_path)
        cursor = conn.cursor()
        
        # è·å–æ‰€æœ‰è¡¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
        tables = [row[0] for row in cursor.fetchall()]
        
        result = {
            "success": True,
            "db_path": db_path,
            "table_count": len(tables),
            "tables": []
        }
        
        if detail:
            for table_name in tables:
                # è·å–è¡¨ç»“æ„
                cursor.execute(f"PRAGMA table_info({table_name})")
                columns = []
                for row in cursor.fetchall():
                    columns.append({
                        "name": row[1],
                        "type": row[2],
                        "notnull": bool(row[3]),
                        "default": row[4],
                        "pk": bool(row[5])
                    })
                
                # è·å–è¡Œæ•°
                cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                row_count = cursor.fetchone()[0]
                
                result["tables"].append({
                    "name": table_name,
                    "row_count": row_count,
                    "columns": columns
                })
        else:
            result["tables"] = [{"name": name} for name in tables]
        
        conn.close()
        return result
        
    except sqlite3.Error as e:
        return {"success": False, "error": f"SQLite error: {str(e)}"}
    except Exception as e:
        return {"success": False, "error": f"Failed to get tables: {str(e)}"}


def _git_init(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆå§‹åŒ– Git ä»“åº“
    
    Params:
        - path: ä»“åº“è·¯å¾„ï¼ˆç›¸å¯¹æ²™ç®±ï¼Œé»˜è®¤å½“å‰ç›®å½•ï¼‰
    """
    try:
        from dulwich.repo import Repo
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä»“åº“
        git_dir = os.path.join(full_path, '.git')
        if os.path.exists(git_dir):
            return {"success": False, "error": "Git repository already exists"}
        
        # åˆå§‹åŒ–ä»“åº“
        Repo.init(full_path)
        
        return {
            "success": True,
            "data": f"Git repository initialized at {repo_path}",
            "stdout": f"ğŸ“ Git ä»“åº“å·²åˆå§‹åŒ–\nğŸ“‚ è·¯å¾„: {repo_path}",
            "path": repo_path
        }
        
    except Exception as e:
        return {"success": False, "error": f"Git init failed: {str(e)}"}


def _git_status(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    è·å– Git ä»“åº“çŠ¶æ€
    
    Params:
        - path: ä»“åº“è·¯å¾„ï¼ˆç›¸å¯¹æ²™ç®±ï¼Œé»˜è®¤å½“å‰ç›®å½•ï¼‰
    """
    try:
        from dulwich.repo import Repo
        from dulwich.porcelain import status
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        # è·å–çŠ¶æ€
        status_result = status(repo)
        
        # è§£æçŠ¶æ€
        staged = {
            'add': list(status_result.staged['add']),
            'delete': list(status_result.staged['delete']),
            'modify': list(status_result.staged['modify'])
        }
        
        unstaged = list(status_result.unstaged)
        untracked = list(status_result.untracked)
        
        staged_count = len(staged['add']) + len(staged['delete']) + len(staged['modify'])
        unstaged_count = len(unstaged)
        untracked_count = len(untracked)
        
        return {
            "success": True,
            "stdout": f"ğŸ“ Git ä»“åº“çŠ¶æ€\nğŸ“¦ æš‚å­˜åŒº: {staged_count} ä¸ª\nğŸ“„ æœªæš‚å­˜: {unstaged_count} ä¸ª\nâ“ æœªè·Ÿè¸ª: {untracked_count} ä¸ª\n{'âœ… å·¥ä½œåŒºå¹²å‡€' if not any([staged['add'], staged['delete'], staged['modify'], unstaged, untracked]) else 'âš ï¸  æœ‰å˜æ›´'}",
            "staged": staged,
            "unstaged": unstaged,
            "untracked": untracked,
            "is_clean": not any([staged['add'], staged['delete'], staged['modify'], unstaged, untracked])
        }
        
    except Exception as e:
        return {"success": False, "error": f"Git status failed: {str(e)}"}


def _git_log(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    è·å– Git æäº¤å†å²
    
    Params:
        - path: ä»“åº“è·¯å¾„ï¼ˆç›¸å¯¹æ²™ç®±ï¼Œé»˜è®¤å½“å‰ç›®å½•ï¼‰
        - max_count: æœ€å¤§è¿”å›æäº¤æ•°ï¼ˆé»˜è®¤ 20ï¼‰
    """
    try:
        from dulwich.repo import Repo
        from dulwich.walk import Walker
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    max_count = params.get('max_count', 20)
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        commits = []
        walker = Walker(repo, repo.head(), max_entries=max_count)
        
        for entry in walker:
            commit = entry.commit
            # è·å– commit SHA
            commit_sha = commit.id
            commits.append({
                "sha": commit_sha.hex()[:7],
                "full_sha": commit_sha.hex(),
                "message": commit.message.decode('utf-8', errors='replace').strip(),
                "author": commit.author.decode('utf-8', errors='replace'),
                "timestamp": commit.commit_time,
                "committer": commit.committer.decode('utf-8', errors='replace')
            })
        
        # ç”Ÿæˆäººç±»å¯è¯»çš„è¾“å‡º
        if commits:
            commit_lines = [f"ğŸ“Œ {c['sha']} - {c['message'][:50]}{'...' if len(c['message']) > 50 else ''}" for c in commits[:5]]
            if len(commits) > 5:
                commit_lines.append(f"  ... è¿˜æœ‰ {len(commits) - 5} ä¸ªæäº¤")
            stdout = f"ğŸ“œ Git æäº¤å†å²\nğŸ“Š å…± {len(commits)} ä¸ªæäº¤\n\n" + "\n".join(commit_lines)
        else:
            stdout = "ğŸ“œ Git æäº¤å†å²\n(æ— æäº¤è®°å½•)"
        
        return {
            "success": True,
            "stdout": stdout,
            "commits": commits,
            "count": len(commits)
        }
        
    except Exception as e:
        return {"success": False, "error": f"Git log failed: {str(e)}"}


def _git_diff(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    è·å–Gitå·®å¼‚ - æ”¯æŒstagedã€unstagedå’Œcachedæ¨¡å¼
    
    Params:
        - path: ä»“åº“è·¯å¾„ï¼ˆç›¸å¯¹æ²™ç®±ï¼Œé»˜è®¤å½“å‰ç›®å½•ï¼‰
        - file_path: æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        - staged: æ¯”è¾ƒHEADå’Œç´¢å¼•ï¼ˆé»˜è®¤Falseï¼Œæ¯”è¾ƒç´¢å¼•å’Œå·¥ä½œåŒºï¼‰
        - cached: åŒstagedï¼Œgitæ ‡å‡†å‚æ•°åˆ«å
    """
    try:
        from dulwich.repo import Repo
        from dulwich.diff_tree import tree_changes
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    file_path = params.get('file_path')
    staged = params.get('staged', False) or params.get('cached', False)
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æäº¤
        try:
            head_commit = repo[repo.head()]
            head_tree = head_commit.tree
        except:
            return {"success": True, "data": "Empty repository - no commits yet", "files": []}
        
        if staged:
            # stagedæ¨¡å¼ï¼šæ¯”è¾ƒ HEAD å’Œ ç´¢å¼•
            return _diff_staged(repo, head_tree, file_path)
        else:
            # unstagedæ¨¡å¼ï¼šæ¯”è¾ƒ ç´¢å¼• å’Œ å·¥ä½œåŒº
            return _diff_unstaged(repo, full_path, head_tree, file_path)
        
    except Exception as e:
        import traceback
        return {"success": False, "error": f"Git diff failed: {str(e)}", "traceback": traceback.format_exc()}


def _diff_staged(repo, head_tree, file_path_filter=None):
    """æ¯”è¾ƒ HEAD å’Œ ç´¢å¼•ï¼ˆstaged changesï¼‰"""
    from dulwich.diff_tree import tree_changes
    
    # è·å–ç´¢å¼•æ ‘
    index = repo.open_index()
    index_tree_id = index.commit(repo.object_store)
    
    # æ¯”è¾ƒ HEAD å’Œ ç´¢å¼•
    changes = list(tree_changes(repo, head_tree, index_tree_id))
    
    diffs = []
    for change in changes:
        old_path, new_path = _get_change_paths(change)
        
        # è¿‡æ»¤æŒ‡å®šæ–‡ä»¶
        if file_path_filter and old_path != file_path_filter and new_path != file_path_filter:
            continue
        
        diffs.append({
            "change_type": _normalize_change_type(change.type),
            "old_path": old_path,
            "new_path": new_path,
            "staged": True
        })
    
    # ç”Ÿæˆäººç±»å¯è¯»çš„è¾“å‡º
    if diffs:
        file_lines = [f"  {_normalize_change_type(change.type)}: {new_path or old_path}" for change in changes]
        stdout = f"ğŸ“Š Staged å˜æ›´ ({len(diffs)} ä¸ªæ–‡ä»¶)\n" + "\n".join(file_lines)
    else:
        stdout = "ğŸ“Š Staged å˜æ›´\n(æ— å˜æ›´)"
    
    return {
        "success": True,
        "stdout": stdout,
        "files": diffs,
        "count": len(diffs),
        "mode": "staged"
    }


def _diff_unstaged(repo, repo_path, head_tree, file_path_filter=None):
    """
    æ¯”è¾ƒ ç´¢å¼• å’Œ å·¥ä½œåŒºï¼ˆunstaged changesï¼‰
    ç®€åŒ–å®ç°ï¼šéå†ç´¢å¼•å’Œå·¥ä½œåŒºï¼Œå¯¹æ¯”SHA
    """
    index = repo.open_index()
    diffs = []
    
    # 1. æ£€æŸ¥ç´¢å¼•ä¸­çš„æ–‡ä»¶ï¼ˆä¿®æ”¹å’Œåˆ é™¤ï¼‰
    for path_bytes, entry in index.items():
        path = path_bytes.decode('utf-8', errors='replace')
        
        # è¿‡æ»¤æŒ‡å®šæ–‡ä»¶
        if file_path_filter and path != file_path_filter:
            continue
        
        full_path = os.path.join(repo_path, path)
        
        if not os.path.exists(full_path):
            # æ–‡ä»¶åœ¨ç´¢å¼•ä¸­ä½†ä¸å­˜åœ¨äºå·¥ä½œåŒº -> åˆ é™¤
            diffs.append({
                "change_type": "delete",
                "old_path": path,
                "new_path": None,
                "staged": False
            })
        else:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¿®æ”¹
            current_sha = _get_file_sha(full_path)
            if current_sha != entry.sha.hex():
                diffs.append({
                    "change_type": "modify",
                    "old_path": path,
                    "new_path": path,
                    "staged": False
                })
    
    # 2. æ£€æŸ¥å·¥ä½œåŒºä¸­çš„æ–°æ–‡ä»¶
    for root, dirs, files in os.walk(repo_path):
        if '.git' in root:
            continue
        
        for file in files:
            rel_path = os.path.relpath(os.path.join(root, file), repo_path)
            path_bytes = rel_path.encode('utf-8')
            
            # è¿‡æ»¤æŒ‡å®šæ–‡ä»¶
            if file_path_filter and rel_path != file_path_filter:
                continue
            
            # æ–‡ä»¶åœ¨å·¥ä½œåŒºä½†ä¸åœ¨ç´¢å¼•ä¸­ -> æ–°å¢
            if path_bytes not in index:
                diffs.append({
                    "change_type": "add",
                    "old_path": None,
                    "new_path": rel_path,
                    "staged": False
                })
    
    # ç”Ÿæˆäººç±»å¯è¯»çš„è¾“å‡º
    if diffs:
        file_lines = []
        for d in diffs[:10]:
            icon = {"add": "â•", "delete": "ğŸ—‘ï¸", "modify": "âœï¸"}.get(d["change_type"], "ğŸ“")
            path = d["new_path"] or d["old_path"]
            file_lines.append(f"  {icon} {path}")
        if len(diffs) > 10:
            file_lines.append(f"  ... è¿˜æœ‰ {len(diffs) - 10} ä¸ªæ–‡ä»¶")
        stdout = f"ğŸ“Š Unstaged å˜æ›´ ({len(diffs)} ä¸ªæ–‡ä»¶)\n" + "\n".join(file_lines)
    else:
        stdout = "ğŸ“Š Unstaged å˜æ›´\n(æ— å˜æ›´)"
    
    return {
        "success": True,
        "stdout": stdout,
        "files": diffs,
        "count": len(diffs),
        "mode": "unstaged"
    }


def _get_file_sha(file_path):
    """è®¡ç®—æ–‡ä»¶çš„SHA1å“ˆå¸Œï¼ˆGité£æ ¼ï¼‰"""
    from hashlib import sha1
    
    try:
        with open(file_path, 'rb') as f:
            content = f.read()
        
        # Gité£æ ¼çš„SHAï¼š'blob <size>\0<content>'
        header = f"blob {len(content)}\0".encode()
        return sha1(header + content).hexdigest()
    except:
        return None


def _get_change_paths(change):
    """å®‰å…¨è·å–å˜æ›´è·¯å¾„"""
    old_path = None
    new_path = None
    
    if change.old and hasattr(change.old, 'path') and change.old.path:
        old_path = change.old.path.decode('utf-8', errors='replace')
    if change.new and hasattr(change.new, 'path') and change.new.path:
        new_path = change.new.path.decode('utf-8', errors='replace')
    
    return old_path, new_path


def _normalize_change_type(change_type):
    """æ ‡å‡†åŒ–å˜æ›´ç±»å‹"""
    # dulwichçš„change type: 'add', 'delete', 'modify', 'unchanged'
    if change_type == 'add':
        return 'add'
    elif change_type == 'delete':
        return 'delete'
    elif change_type == 'modify':
        return 'modify'
    else:
        return change_type


def _git_add(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Add files to Git staging area
    
    Params:
        - path: Repository path (relative to sandbox, default current dir)
        - file_path: File path (relative to repo), supports wildcards like '*.txt', or '.' for all
    """
    try:
        from dulwich.repo import Repo
        from dulwich.porcelain import add
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    file_path = params.get('file_path')
    
    if not file_path:
        return {"success": False, "error": "Missing file_path parameter"}
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        import fnmatch
        full_file_path = _validate_path(full_path, file_path)
        
        files_to_add = []
        if file_path == '.':
            for root, dirs, files in os.walk(full_path):
                if '.git' in root:
                    continue
                for file in files:
                    if file == '.git':
                        continue
                    rel_path = os.path.relpath(os.path.join(root, file), full_path)
                    files_to_add.append(rel_path.encode('utf-8'))
        elif '*' in file_path or '?' in file_path:
            for root, dirs, files in os.walk(full_path):
                if '.git' in root:
                    continue
                for file in files:
                    rel_path = os.path.relpath(os.path.join(root, file), full_path)
                    if fnmatch.fnmatch(rel_path, file_path):
                        files_to_add.append(rel_path.encode('utf-8'))
        else:
            if os.path.exists(full_file_path):
                rel_path = os.path.relpath(full_file_path, full_path)
                files_to_add.append(rel_path.encode('utf-8'))
            else:
                return {"success": False, "error": f"File not found: {file_path}"}
        
        if not files_to_add:
            return {
                "success": True,
                "data": "No files to add",
                "stdout": "â• Git æ·»åŠ \nğŸ“Š æ²¡æœ‰æ–‡ä»¶éœ€è¦æ·»åŠ ",
                "files_added": []
            }
        
        add(repo, files_to_add)
        
        added_files = [f.decode('utf-8', errors='replace') for f in files_to_add]
        return {
            "success": True,
            "data": f"Added {len(files_to_add)} file(s) to staging area",
            "stdout": f"â• Git æ·»åŠ \nğŸ“Š æ·»åŠ äº† {len(added_files)} ä¸ªæ–‡ä»¶\n  " + "\n  ".join(added_files[:10]) + (f"\n  ... è¿˜æœ‰ {len(added_files) - 10} ä¸ªæ–‡ä»¶" if len(added_files) > 10 else ""),
            "files_added": added_files
        }
        
    except Exception as e:
        return {"success": False, "error": f"Git add failed: {str(e)}"}


def _git_commit(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Commit staged changes
    
    Params:
        - path: Repository path (relative to sandbox, default current dir)
        - message: Commit message (required)
        - author_name: Author name (optional, default "RikkaHub User")
        - author_email: Author email (optional, default "user@rikkahub.local")
    """
    try:
        from dulwich.repo import Repo
        from dulwich.porcelain import commit
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    message = params.get('message')
    author_name = params.get('author_name', 'RikkaHub User')
    author_email = params.get('author_email', 'user@rikkahub.local')
    
    if not message:
        return {"success": False, "error": "Missing message parameter"}
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        from dulwich.porcelain import status
        status_result = status(repo)
        if not any([status_result.staged['add'], status_result.staged['delete'], status_result.staged['modify']]):
            return {
                "success": False,
                "error": "No changes staged for commit",
                "stdout": "âŒ Git æäº¤å¤±è´¥\nğŸ“ æ²¡æœ‰æš‚å­˜çš„å˜æ›´éœ€è¦æäº¤"
            }
        
        author = f"{author_name} <{author_email}>".encode('utf-8')
        
        commit_sha = commit(
            repo,
            message=message.encode('utf-8'),
            author=author,
            committer=author
        )
        
        short_sha = commit_sha.hex()[:7]
        return {
            "success": True,
            "data": f"Committed: {message}",
            "stdout": f"âœ… Git æäº¤æˆåŠŸ\nğŸ“ æäº¤æ¶ˆæ¯: {message}\nğŸ”– æäº¤ SHA: {short_sha}\nğŸ‘¤ ä½œè€…: {author_name}",
            "commit_sha": short_sha,
            "full_sha": commit_sha.hex(),
            "author": f"{author_name} <{author_email}>"
        }
        
    except Exception as e:
        return {"success": False, "error": f"Git commit failed: {str(e)}"}


def _checkout_file(repo, repo_path: str, file_path: str) -> Dict[str, Any]:
    """ä»HEADæ¢å¤ç‰¹å®šæ–‡ä»¶"""
    try:
        # è·å–HEADæäº¤
        head_sha = repo.head()
        commit = repo[head_sha]
        tree = repo[commit.tree]
        
        # æŸ¥æ‰¾æ–‡ä»¶
        file_path_bytes = file_path.encode('utf-8')
        if file_path_bytes not in tree:
            return {"success": False, "error": f"File '{file_path}' not found in HEAD"}
        
        # è·å–æ–‡ä»¶å†…å®¹
        entry = tree[file_path_bytes]
        blob = repo[entry.sha]
        content = blob.as_raw_string()
        
        # å†™å…¥æ–‡ä»¶
        full_file_path = os.path.join(repo_path, file_path)
        os.makedirs(os.path.dirname(full_file_path) or '.', exist_ok=True)
        with open(full_file_path, 'wb') as f:
            f.write(content)
        
        return {
            "success": True,
            "data": f"Restored '{file_path}' from HEAD",
            "stdout": f"ğŸ“„ Git æ–‡ä»¶æ¢å¤\nâœ… å·²æ¢å¤æ–‡ä»¶: {file_path}\nğŸ“ å¤§å°: {len(content)} bytes",
            "file_path": file_path,
            "bytes_written": len(content)
        }
        
    except Exception as e:
        return {"success": False, "error": f"File checkout failed: {str(e)}"}


def _checkout_branch(repo, repo_path: str, branch_name: str, create: bool) -> Dict[str, Any]:
    """åˆ‡æ¢åˆ°æŒ‡å®šåˆ†æ”¯ï¼Œå®Œå…¨é‡ç½®å·¥ä½œåŒº"""
    from dulwich.index import build_index_from_tree
    
    branch_ref = f"refs/heads/{branch_name}".encode('utf-8')
    
    # æ£€æŸ¥åˆ†æ”¯æ˜¯å¦å­˜åœ¨
    if branch_ref not in repo.refs:
        if create:
            # åˆ›å»ºæ–°åˆ†æ”¯
            try:
                head_sha = repo.head()
                repo.refs[branch_ref] = head_sha
            except Exception as e:
                return {"success": False, "error": f"Cannot create branch: {str(e)}"}
        else:
            return {"success": False, "error": f"Branch '{branch_name}' not found"}
    
    # è·å–ç›®æ ‡åˆ†æ”¯çš„commitå’Œtree
    target_sha = repo.refs[branch_ref]
    commit = repo[target_sha]
    tree_sha = commit.tree
    
    # åˆ‡æ¢HEAD
    repo.refs.set_symbolic_ref(b'HEAD', branch_ref)
    
    # é‡ç½®ç´¢å¼•ï¼šä½¿ç”¨build_index_from_treeé‡å»ºç´¢å¼•
    index_path = os.path.join(repo_path, '.git', 'index')
    build_index_from_tree(repo.path, index_path, repo.object_store, tree_sha)
    
    # é‡ç½®å·¥ä½œåŒºï¼šéå†treeï¼Œå°†æ¯ä¸ªblobå†™å…¥æ–‡ä»¶ç³»ç»Ÿ
    _reset_working_tree(repo, repo_path, tree_sha)
    
    # åˆ é™¤ç›®æ ‡åˆ†æ”¯æ²¡æœ‰ä½†å·¥ä½œåŒºæœ‰çš„æ–‡ä»¶
    _remove_untracked_files(repo, repo_path, tree_sha)
    
    action_msg = "åˆ›å»ºå¹¶åˆ‡æ¢åˆ°" if create else "åˆ‡æ¢åˆ°"
    return {
        "success": True,
        "data": f"{action_msg} branch '{branch_name}' and reset working tree",
        "stdout": f"ğŸŒ¿ Git åˆ†æ”¯åˆ‡æ¢\nâœ… å·²{action_msg}: {branch_name}\nğŸ”– SHA: {target_sha.hex()[:7]}\nğŸ“Š å·¥ä½œåŒºå·²é‡ç½®",
        "branch": branch_name,
        "commit_sha": target_sha.hex()[:7],
        "files_reset": True
    }


def _reset_working_tree(repo, repo_path: str, tree_sha):
    """æ ¹æ®treeå¯¹è±¡é‡ç½®å·¥ä½œåŒºæ–‡ä»¶"""
    tree = repo[tree_sha]
    
    for entry in tree.items():
        path = entry.path.decode('utf-8', errors='replace')
        full_path = os.path.join(repo_path, path)
        
        if entry.mode == 0o040000:  # ç›®å½•
            os.makedirs(full_path, exist_ok=True)
        else:  # æ–‡ä»¶
            # è·å–blobå†…å®¹
            blob = repo[entry.sha]
            content = blob.as_raw_string()
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(full_path) or '.', exist_ok=True)
            
            # å†™å…¥æ–‡ä»¶
            with open(full_path, 'wb') as f:
                f.write(content)


def _remove_untracked_files(repo, repo_path: str, tree_sha):
    """åˆ é™¤å·¥ä½œåŒºä¸­ä½†ä¸åœ¨treeä¸­çš„æ–‡ä»¶"""
    # è·å–treeä¸­æ‰€æœ‰æ–‡ä»¶è·¯å¾„
    tree = repo[tree_sha]
    tracked_files = set()
    
    def collect_paths(tree_obj, prefix=''):
        for entry in tree_obj.items():
            path = os.path.join(prefix, entry.path.decode('utf-8', errors='replace'))
            if entry.mode == 0o040000:  # ç›®å½•
                sub_tree = repo[entry.sha]
                collect_paths(sub_tree, path)
            else:
                tracked_files.add(path)
    
    collect_paths(tree)
    
    # éå†å·¥ä½œåŒºï¼Œåˆ é™¤æœªè·Ÿè¸ªçš„æ–‡ä»¶
    for root, dirs, files in os.walk(repo_path):
        # è·³è¿‡.gitç›®å½•
        if '.git' in root:
            continue
        
        for file in files:
            rel_path = os.path.relpath(os.path.join(root, file), repo_path)
            if rel_path not in tracked_files:
                try:
                    os.remove(os.path.join(root, file))
                except:
                    pass


def _git_branch(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Branch management: list, create, delete branches
    
    Params:
        - path: Repository path (relative to sandbox, default current dir)
        - action: Operation type - "list", "create", "delete" (default "list")
        - branch_name: Branch name (required for create/delete)
        - checkout: Switch to new branch after creation (optional, default False)
    """
    try:
        from dulwich.repo import Repo
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    action = params.get('action', 'list')
    branch_name = params.get('branch_name')
    checkout = params.get('checkout', False)
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        if action == 'list':
            branches = []
            current_branch = None
            
            try:
                current_ref = repo.refs.read_ref(b'HEAD')
                if current_ref.startswith(b'ref: refs/heads/'):
                    current_branch = current_ref[16:].decode('utf-8', errors='replace')
            except:
                pass
            
            for ref in repo.refs.keys():
                if ref.startswith(b'refs/heads/'):
                    name = ref[11:].decode('utf-8', errors='replace')
                    sha = repo.refs[ref].hex()[:7]
                    branches.append({
                        "name": name,
                        "sha": sha,
                        "is_current": name == current_branch
                    })
            
            # æ„å»ºstdout
            branch_lines = []
            for b in branches:
                icon = "ğŸ‘‰" if b["name"] == current_branch else "  "
                branch_lines.append(f"{icon} {b['name']} ({b['sha']})")
            
            return {
                "success": True,
                "stdout": f"ğŸŒ¿ Git åˆ†æ”¯åˆ—è¡¨\nğŸ“Š å…± {len(branches)} ä¸ªåˆ†æ”¯\n" + ("\n".join(branch_lines) if branch_lines else "  (æ— åˆ†æ”¯)"),
                "branches": branches,
                "current_branch": current_branch,
                "count": len(branches)
            }
        
        elif action == 'create':
            if not branch_name:
                return {"success": False, "error": "Missing branch_name parameter"}
            
            branch_ref = f"refs/heads/{branch_name}".encode('utf-8')
            
            if branch_ref in repo.refs:
                return {"success": False, "error": f"Branch '{branch_name}' already exists"}
            
            try:
                head_sha = repo.head()
                repo.refs[branch_ref] = head_sha
            except Exception as e:
                return {"success": False, "error": f"Cannot create branch: {str(e)}"}
            
            if checkout:
                # ä½¿ç”¨å®Œæ•´çš„checkouté€»è¾‘é‡ç½®å·¥ä½œåŒº
                return _checkout_branch(repo, full_path, branch_name, create=False)
            
            return {
                "success": True,
                "data": f"Created branch '{branch_name}'",
                "stdout": f"âœ… Git åˆ›å»ºåˆ†æ”¯æˆåŠŸ\nğŸŒ¿ åˆ†æ”¯å: {branch_name}",
                "branch": branch_name
            }
        
        elif action == 'checkout':
            # ç›´æ¥è°ƒç”¨checkouté€»è¾‘
            if not branch_name:
                return {"success": False, "error": "Missing branch_name parameter"}
            return _checkout_branch(repo, full_path, branch_name, create=False)
        
        elif action == 'delete':
            if not branch_name:
                return {"success": False, "error": "Missing branch_name parameter"}
            
            branch_ref = f"refs/heads/{branch_name}".encode('utf-8')
            
            if branch_ref not in repo.refs:
                return {"success": False, "error": f"Branch '{branch_name}' not found"}
            
            current_ref = repo.refs.read_ref(b'HEAD')
            if current_ref == b'ref: ' + branch_ref:
                return {"success": False, "error": f"Cannot delete current branch '{branch_name}'"}
            
            del repo.refs[branch_ref]
            
            return {
                "success": True,
                "data": f"Deleted branch '{branch_name}'",
                "stdout": f"âœ… Git åˆ é™¤åˆ†æ”¯æˆåŠŸ\nğŸ—‘ï¸ å·²åˆ é™¤åˆ†æ”¯: {branch_name}",
                "branch_deleted": branch_name
            }
        
        else:
            return {"success": False, "error": f"Unknown action: {action}"}
        
    except Exception as e:
        return {"success": False, "error": f"Git branch failed: {str(e)}"}


def _git_checkout(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Switch branches or restore files with full working tree reset
    
    Params:
        - path: Repository path (relative to sandbox, default current dir)
        - branch_name: Branch name (alternative to file_path)
        - file_path: Restore specific file (alternative to branch_name)
        - create: Create branch if not exists (optional, default False, like git checkout -b)
    """
    try:
        from dulwich.repo import Repo
        from dulwich.objects import Tree
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    branch_name = params.get('branch_name')
    file_path = params.get('file_path')
    create = params.get('create', False)
    
    if not branch_name and not file_path:
        return {"success": False, "error": "Must specify branch_name or file_path"}
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        # åœºæ™¯1: æ¢å¤ç‰¹å®šæ–‡ä»¶
        if file_path and not branch_name:
            return _checkout_file(repo, full_path, file_path)
        
        # åœºæ™¯2: åˆ‡æ¢åˆ†æ”¯
        return _checkout_branch(repo, full_path, branch_name, create)
        
    except Exception as e:
        import traceback
        return {"success": False, "error": f"Git checkout failed: {str(e)}", "traceback": traceback.format_exc()}


def _git_rm(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Remove files from Git staging area (git rm)
    
    Params:
        - path: Repository path (relative to sandbox, default current dir)
        - file_path: File path to remove
        - cached: Only remove from staging, keep file (optional, default False)
    """
    try:
        from dulwich.repo import Repo
        from dulwich.index import Index
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    file_path = params.get('file_path')
    cached = params.get('cached', False)
    
    if not file_path:
        return {"success": False, "error": "Missing file_path parameter"}
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        full_file_path = _validate_path(full_path, file_path)
        rel_path = os.path.relpath(full_file_path, full_path)
        
        index = repo.open_index()
        path_bytes = rel_path.encode('utf-8')
        
        if path_bytes in index:
            del index[path_bytes]
        
        # Write the index back
        index.write()
        
        # If not cached, also remove the file from disk
        file_deleted = False
        if not cached and os.path.exists(full_file_path):
            os.remove(full_file_path)
            file_deleted = True
        
        action = "ä»æš‚å­˜åŒºç§»é™¤" if cached else "åˆ é™¤"
        return {
            "success": True,
            "stdout": f"ğŸ—‘ï¸ Git åˆ é™¤\nâœ… å·²{action}: {file_path}" + ("\nğŸ“„ æ–‡ä»¶å·²ä»ç£ç›˜åˆ é™¤" if file_deleted else "\nğŸ“„ æ–‡ä»¶ä»ä¿ç•™åœ¨ç£ç›˜")
        }
    except Exception as e:
        return {"success": False, "error": str(e)}


def _git_checkpoint(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Create a workflow checkpoint (git commit with special message)
    
    This operation stages all changes and creates a commit with a special prefix
    to identify it as a workflow checkpoint.
    
    Params:
        - path: Repository path (relative to sandbox, default current dir)
        - message: Checkpoint message (optional, default "Workflow checkpoint")
        - bound_message_index: Index of the message this checkpoint is bound to (optional)
    
    Returns:
        - success: True if checkpoint created
        - checkpoint_id: Git commit hash (short)
        - full_sha: Full git commit hash
        - bound_message_index: Message index (if provided)
    """
    try:
        from dulwich.repo import Repo
        from dulwich.porcelain import commit, status
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    message = params.get('message', 'Workflow checkpoint')
    bound_message_index = params.get('bound_message_index')
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        # Stage all changes
        from dulwich.porcelain import add
        add(repo, paths=['.'])
        
        # Check if there are changes to commit
        status_result = status(repo)
        has_changes = any([
            status_result.staged.get('add', []),
            status_result.staged.get('delete', []),
            status_result.staged.get('modify', [])
        ])
        
        # Build checkpoint message with metadata
        checkpoint_message = f"[WORKFLOW-CHECKPOINT] {message}"
        if bound_message_index is not None:
            checkpoint_message += f" | message_index={bound_message_index}"
        
        author = "RikkaHub Workflow <workflow@rikkahub.local>".encode('utf-8')
        
        if has_changes:
            commit_sha = commit(
                repo,
                message=checkpoint_message.encode('utf-8'),
                author=author,
                committer=author
            )
        else:
            # No changes, just get current HEAD
            commit_sha = repo.head()
        
        short_sha = commit_sha.hex()[:7]
        return {
            "success": True,
            "stdout": f"ğŸ¯ Git æ£€æŸ¥ç‚¹\nâœ… å·²åˆ›å»ºæ£€æŸ¥ç‚¹\nğŸ“ æ¶ˆæ¯: {message}\nğŸ”– SHA: {short_sha}" + (f"\nğŸ“ ç»‘å®šæ¶ˆæ¯ç´¢å¼•: {bound_message_index}" if bound_message_index is not None else "") + ("\nâš ï¸ æ²¡æœ‰å˜æ›´ï¼Œä½¿ç”¨å½“å‰ HEAD" if not has_changes else ""),
            "checkpoint_id": short_sha,
            "full_sha": commit_sha.hex(),
            "message": checkpoint_message,
            "bound_message_index": bound_message_index,
            "has_changes": has_changes
        }
        
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": f"Git checkpoint failed: {str(e)}",
            "traceback": traceback.format_exc()
        }


def _git_restore(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Restore to a previous checkpoint (git reset --hard)
    
    This operation resets the working directory and index to the specified commit.
    
    Params:
        - path: Repository path (relative to sandbox, default current dir)
        - checkpoint_id: Git commit hash (short or full) to restore to
        - clean: Remove untracked files after restore (optional, default True)
    
    Returns:
        - success: True if restore successful
        - restored_to: Git commit hash that was restored
    """
    try:
        from dulwich.repo import Repo
        from dulwich.objects import Commit, Tree
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    checkpoint_id = params.get('checkpoint_id')
    clean = params.get('clean', True)
    
    if not checkpoint_id:
        return {"success": False, "error": "Missing checkpoint_id parameter"}
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        # Resolve checkpoint_id (handle short hashes)
        try:
            # Try as full SHA first
            target_sha = bytes.fromhex(checkpoint_id)
            if target_sha not in repo.object_store:
                # Try as short hash
                for obj_id in repo.object_store:
                    if obj_id.hex().startswith(checkpoint_id.lower()):
                        target_sha = obj_id
                        break
                else:
                    return {"success": False, "error": f"Checkpoint not found: {checkpoint_id}"}
        except ValueError:
            return {"success": False, "error": f"Invalid checkpoint ID: {checkpoint_id}"}
        
        # Verify it's a commit
        try:
            commit_obj = repo[target_sha]
            if not isinstance(commit_obj, Commit):
                return {"success": False, "error": f"Not a commit: {checkpoint_id}"}
        except KeyError:
            return {"success": False, "error": f"Checkpoint not found: {checkpoint_id}"}
        
        # Reset HEAD to target commit
        repo.refs[b'HEAD'] = target_sha
        
        # Reset working directory
        tree = repo[commit_obj.tree]
        
        # Remove all tracked files
        for root, dirs, files in os.walk(full_path):
            # Skip .git directory
            if '.git' in root:
                continue
            
            for file in files:
                rel_path = os.path.relpath(os.path.join(root, file), full_path)
                path_bytes = rel_path.encode('utf-8')
                
                # Check if file is in the new tree
                try:
                    if path_bytes not in tree:
                        # File not in new tree, delete it
                        os.remove(os.path.join(root, file))
                except:
                    pass
        
        # Write files from new tree
        def write_tree_to_path(tree_obj, base_path):
            for name, entry in tree_obj.items():
                entry_path = os.path.join(base_path, name.decode('utf-8'))
                obj = repo[entry.sha]
                
                if isinstance(obj, Tree):
                    os.makedirs(entry_path, exist_ok=True)
                    write_tree_to_path(obj, entry_path)
                else:
                    # Blob - write file
                    os.makedirs(os.path.dirname(entry_path), exist_ok=True)
                    with open(entry_path, 'wb') as f:
                        f.write(obj.as_raw_string())
        
        write_tree_to_path(tree, full_path)
        
        # Clean untracked files if requested
        if clean:
            # Get tracked files from new tree
            tracked_files = set()
            def collect_tracked(tree_obj, prefix=''):
                for name, entry in tree_obj.items():
                    path = prefix + name.decode('utf-8')
                    tracked_files.add(path)
                    obj = repo[entry.sha]
                    if isinstance(obj, Tree):
                        collect_tracked(obj, path + '/')
            collect_tracked(tree)
            
            # Remove untracked files
            for root, dirs, files in os.walk(full_path):
                if '.git' in root:
                    continue
                
                for file in files:
                    rel_path = os.path.relpath(os.path.join(root, file), full_path)
                    if rel_path not in tracked_files:
                        try:
                            os.remove(os.path.join(root, file))
                        except:
                            pass
        
        return {
            "success": True,
            "stdout": f"ğŸ”„ Git æ¢å¤\nâœ… å·²æ¢å¤åˆ°æ£€æŸ¥ç‚¹\nğŸ”– SHA: {target_sha.hex()[:7]}" + ("\nğŸ§¹ å·²æ¸…ç†æœªè·Ÿè¸ªæ–‡ä»¶" if clean else ""),
            "restored_to": checkpoint_id,
            "full_sha": target_sha.hex(),
            "clean": clean
        }
        
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": f"Git restore failed: {str(e)}",
            "traceback": traceback.format_exc()
        }


def _git_list_checkpoints(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    List all workflow checkpoints
    
    This operation lists all commits with the [WORKFLOW-CHECKPOINT] prefix.
    
    Params:
        - path: Repository path (relative to sandbox, default current dir)
        - limit: Maximum number of checkpoints to return (optional, default 50)
    
    Returns:
        - success: True if listing successful
        - checkpoints: List of checkpoint objects
    """
    try:
        from dulwich.repo import Repo
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    limit = params.get('limit', 50)
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        # Walk commit history
        checkpoints = []
        seen = set()
        walker = repo.get_walker(max_entries=limit * 2)  # Get more to filter
        
        for entry in walker:
            commit = entry.commit
            sha = commit.id.hex()
            
            if sha in seen:
                continue
            seen.add(sha)
            
            message = commit.message.decode('utf-8', errors='replace')
            
            # Check if it's a workflow checkpoint
            if '[WORKFLOW-CHECKPOINT]' in message:
                # Parse metadata
                bound_index = None
                if 'message_index=' in message:
                    try:
                        idx_part = message.split('message_index=')[1].split()[0]
                        bound_index = int(idx_part)
                    except:
                        pass
                
                # Extract clean message
                clean_message = message.replace('[WORKFLOW-CHECKPOINT]', '').strip()
                if '|' in clean_message:
                    clean_message = clean_message.split('|')[0].strip()
                
                checkpoints.append({
                    "checkpoint_id": sha[:7],
                    "full_sha": sha,
                    "message": clean_message,
                    "bound_message_index": bound_index,
                    "timestamp": commit.commit_time
                })
                
                if len(checkpoints) >= limit:
                    break
        
        # æ„å»ºstdout
        checkpoint_lines = []
        for cp in checkpoints[:10]:
            time_str = f" ({cp['timestamp']})" if cp['timestamp'] else ""
            bound_info = f" [msg#{cp['bound_message_index']}]" if cp['bound_message_index'] is not None else ""
            checkpoint_lines.append(f"  ğŸ¯ {cp['checkpoint_id']}{time_str} - {cp['message']}{bound_info}")
        if len(checkpoints) > 10:
            checkpoint_lines.append(f"  ... è¿˜æœ‰ {len(checkpoints) - 10} ä¸ªæ£€æŸ¥ç‚¹")
        
        return {
            "success": True,
            "stdout": f"ğŸ“‹ Git æ£€æŸ¥ç‚¹åˆ—è¡¨\nğŸ“Š å…± {len(checkpoints)} ä¸ªæ£€æŸ¥ç‚¹\n" + ("\n".join(checkpoint_lines) if checkpoint_lines else "  (æ— æ£€æŸ¥ç‚¹)"),
            "checkpoints": checkpoints,
            "count": len(checkpoints)
        }
        
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": f"Git list checkpoints failed: {str(e)}",
            "traceback": traceback.format_exc()
        }


def _git_rm(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Remove files from Git staging area (git rm)
    
    Params:
        - path: Repository path (relative to sandbox, default current dir)
        - file_path: File path to remove
        - cached: Only remove from staging, keep file (optional, default False)
    """
    try:
        from dulwich.repo import Repo
        from dulwich.index import Index
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    file_path = params.get('file_path')
    cached = params.get('cached', False)
    
    if not file_path:
        return {"success": False, "error": "Missing file_path parameter"}
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        full_file_path = _validate_path(full_path, file_path)
        rel_path = os.path.relpath(full_file_path, full_path)
        
        index = repo.open_index()
        path_bytes = rel_path.encode('utf-8')
        
        if path_bytes in index:
            del index[path_bytes]
        
        index.write()
        
        file_deleted = False
        if not cached and os.path.exists(full_file_path):
            os.remove(full_file_path)
            file_deleted = True
        
        return {
            "success": True,
            "data": f"Removed '{file_path}' from staging area",
            "file_deleted": file_deleted
        }
        
    except Exception as e:
        return {"success": False, "error": f"Git rm failed: {str(e)}"}


def _git_mv(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    Move or rename Git tracked files (git mv)
    
    Params:
        - path: Repository path (relative to sandbox, default current dir)
        - src: Source file path
        - dst: Destination file path
    """
    try:
        from dulwich.repo import Repo
        from dulwich.index import Index
    except ImportError:
        return {"success": False, "error": "dulwich not available"}
    
    repo_path = params.get('path', '.')
    src = params.get('src')
    dst = params.get('dst')
    
    if not src or not dst:
        return {"success": False, "error": "Missing src or dst parameter"}
    
    try:
        full_path = _validate_path(sandbox_path, repo_path)
        repo = Repo(full_path)
        
        full_src = _validate_path(full_path, src)
        full_dst = _validate_path(full_path, dst)
        
        rel_src = os.path.relpath(full_src, full_path).encode('utf-8')
        rel_dst = os.path.relpath(full_dst, full_path).encode('utf-8')
        
        index = repo.open_index()
        if rel_src not in index:
            return {"success": False, "error": f"'{src}' is not tracked by git"}
        
        os.makedirs(os.path.dirname(full_dst) or '.', exist_ok=True)
        shutil.move(full_src, full_dst)
        
        index[rel_dst] = index[rel_src]
        del index[rel_src]
        index.write()
        
        return {
            "success": True,
            "data": f"Renamed '{src}' to '{dst}'",
            "stdout": f"ğŸ“¦ Git ç§»åŠ¨/é‡å‘½å\nâœ… å·²é‡å‘½å:\n  ä»: {src}\n  åˆ°: {dst}",
            "src": src,
            "dst": dst
        }
        
    except Exception as e:
        return {"success": False, "error": f"Git mv failed: {str(e)}"}


def _exec_script(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œ Shell è„šæœ¬ï¼ˆæ”¯æŒå¤šè¡Œï¼Œç”¨ç³»ç»Ÿ shï¼‰
    å¯ä»¥å¤§å¹…å‡å°‘ AI è°ƒç”¨æ¬¡æ•°ï¼Œå°†å¤šä¸ªå‘½ä»¤åˆå¹¶ä¸ºä¸€æ¬¡æ‰§è¡Œ
    
    Params:
        - script: è„šæœ¬å†…å®¹å­—ç¬¦ä¸²ï¼ˆå¤šè¡Œï¼Œä¼˜å…ˆçº§é«˜äº script_pathï¼‰
        - script_path: è„šæœ¬æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æ²™ç®±ï¼Œå¯é€‰ï¼‰
        - env: ç¯å¢ƒå˜é‡å­—å…¸ï¼ˆå¯é€‰ï¼‰
        - timeout: è¶…æ—¶ç§’æ•°ï¼ˆé»˜è®¤ 60ï¼‰
        
    Examples:
        # æ‰¹é‡å¤„ç†æ–‡ä»¶
        {"script": "for f in *.txt; do echo 'Processing:' $f; done"}
        
        # é“¾å¼æ–‡æœ¬å¤„ç†
        {"script": "cat data.csv | grep ERROR | wc -l"}
        
        # å¤šæ­¥éª¤è‡ªåŠ¨åŒ–
        {"script": "mkdir -p output && for f in *.jpg; do convert $f output/${f%.jpg}.png; done"}
    """
    script_content = params.get('script')
    script_path = params.get('script_path')
    env_vars = params.get('env', {})
    timeout = params.get('timeout', 60)
    
    if not script_content and not script_path:
        return {"success": False, "error": "Missing required parameter: script or script_path"}
    
    try:
        # ä»æ–‡ä»¶è¯»å–è„šæœ¬
        if not script_content and script_path:
            full_script_path = _validate_path(sandbox_path, script_path)
            if not os.path.exists(full_script_path):
                return {"success": False, "error": f"Script file not found: {script_path}"}
            with open(full_script_path, 'r', encoding='utf-8') as f:
                script_content = f.read()
        
        # å®‰å…¨æ£€æŸ¥ - ç¦æ­¢å±é™©å‘½ä»¤
        dangerous_patterns = [
            'rm -rf /', 'rm -rf /*', '> /dev/sda', 'dd if=/dev/zero',
            'mkfs.', 'reboot', 'shutdown', 'poweroff'
        ]
        for pattern in dangerous_patterns:
            if pattern in script_content.lower():
                return {"success": False, "error": f"Security check failed: forbidden pattern '{pattern}'"}
        
        # å‡†å¤‡ç¯å¢ƒå˜é‡
        env = os.environ.copy() if hasattr(os, 'environ') else {}
        env['HOME'] = sandbox_path
        env['PWD'] = sandbox_path
        env['TMPDIR'] = os.path.join(sandbox_path, '.tmp')
        env.update(env_vars)
        
        # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
        os.makedirs(env['TMPDIR'], exist_ok=True)
        
        # å†™å…¥ä¸´æ—¶è„šæœ¬æ–‡ä»¶
        temp_script = os.path.join(sandbox_path, '.tmp', 'exec_script.sh')
        with open(temp_script, 'w', encoding='utf-8') as f:
            f.write('#!/system/bin/sh\n')
            f.write('set -e\n')  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º
            f.write(script_content)
        
        # æ‰§è¡Œè„šæœ¬
        result = subprocess.run(
            ['/system/bin/sh', temp_script],
            cwd=sandbox_path,
            capture_output=True,
            text=True,
            timeout=timeout,
            env=env
        )
        
        # æ¸…ç†ä¸´æ—¶è„šæœ¬
        try:
            os.remove(temp_script)
        except:
            pass
        
        return {
            "success": result.returncode == 0,
            "stdout": result.stdout,
            "stderr": result.stderr if result.stderr else None,
            "return_code": result.returncode,
            "lines_executed": len([l for l in script_content.split('\n') if l.strip() and not l.strip().startswith('#')])
        }
        
    except subprocess.TimeoutExpired:
        return {"success": False, "error": f"Script execution timed out after {timeout} seconds"}
    except Exception as e:
        return {"success": False, "error": f"Script execution failed: {str(e)}"}


def _exec_javascript(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œ JavaScript ä»£ç 
    
    ä½¿ç”¨ PyMiniRacer (åµŒå…¥å¼ V8) æˆ– pyjsparser è¿›è¡Œ AST åˆ†æ
    ä¸»è¦ç”¨äºï¼šæ•°æ®è½¬æ¢ã€JSON å¤„ç†ã€ç®—æ³•éªŒè¯
    
    Params:
        - code: JavaScript ä»£ç å­—ç¬¦ä¸²
        - timeout: è¶…æ—¶ç§’æ•°ï¼ˆé»˜è®¤ 30ï¼‰
        
    Examples:
        {"code": "JSON.stringify({a: 1, b: 2})"}
        {"code": "[1,2,3].map(x => x * 2).join(',')"}
    """
    code = params.get('code')
    timeout = params.get('timeout', 30)
    
    if not code:
        return {"success": False, "error": "Missing code parameter"}
    
    try:
        # å°è¯•ä½¿ç”¨ PyMiniRacer (å¦‚æœå·²å®‰è£…)
        try:
            from py_mini_racer import MiniRacer
            ctx = MiniRacer()
            result = ctx.eval(code)
            return {
                "success": True,
                "result": result,
                "language": "javascript"
            }
        except ImportError:
            pass
        
        # å¤‡é€‰ï¼šä½¿ç”¨ pyjsparser è¿›è¡Œ AST åˆ†æ + ç®€å•è¡¨è¾¾å¼æ±‚å€¼
        import json
        import re
        
        # ç®€å•çš„è¡¨è¾¾å¼æ±‚å€¼å™¨ (æ”¯æŒå¸¸è§åœºæ™¯)
        # æ›¿æ¢ JavaScript è¯­æ³•ä¸º Python
        py_code = code
        py_code = re.sub(r'const\s+', '', py_code)
        py_code = re.sub(r'let\s+', '', py_code)
        py_code = re.sub(r'var\s+', '', py_code)
        py_code = re.sub(r'console\.log\s*\(', 'print(', py_code)
        py_code = re.sub(r'JSON\.stringify\s*\(', 'json.dumps(', py_code)
        py_code = re.sub(r'JSON\.parse\s*\(', 'json.loads(', py_code)
        py_code = re.sub(r'null', 'None', py_code)
        py_code = re.sub(r'true', 'True', py_code)
        py_code = re.sub(r'false', 'False', py_code)
        
        # æ‰§è¡Œè½¬æ¢åçš„ä»£ç 
        exec_globals = {'json': json, 'print': print}
        exec_locals = {}
        
        import io
        import sys
        old_stdout = sys.stdout
        stdout_capture = io.StringIO()
        sys.stdout = stdout_capture
        
        try:
            exec(compile(py_code, '<javascript>', 'exec'), exec_globals, exec_locals)
        finally:
            sys.stdout = old_stdout
        
        output = stdout_capture.getvalue()
        
        return {
            "success": True,
            "stdout": output,
            "transpiled_python": py_code,
            "note": "JavaScript executed via Python transpilation (PyMiniRacer not installed)"
        }
        
    except Exception as e:
        return {"success": False, "error": f"JavaScript execution failed: {str(e)}"}


def _exec_lua(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ‰§è¡Œ/åˆ†æ Lua ä»£ç 
    
    ç”±äº Chaquopy ä¸Š Lua è§£é‡Šå™¨ä¸å¯ç”¨ï¼Œç›®å‰æä¾›ï¼š
    1. è¯­æ³•æ£€æŸ¥å’ŒéªŒè¯
    2. è½¬æ¢ä¸º Python æ‰§è¡Œï¼ˆç®€åŒ–ç‰ˆï¼‰
    3. ä»£ç ç»“æ„åˆ†æ
    
    é€‚åˆï¼šé…ç½®æ–‡ä»¶å¤„ç†ã€ä»£ç åˆ†æã€å­¦ä¹  Lua
    
    Params:
        - code: Lua ä»£ç å­—ç¬¦ä¸²
        - script_path: æˆ–ä»æ–‡ä»¶è¯»å–
        - transpile: æ˜¯å¦è½¬æ¢ä¸º Python æ‰§è¡Œï¼ˆé»˜è®¤ Falseï¼‰
        
    Examples:
        {"code": "return 1 + 2"}
        {"code": "for i=1,10 do print(i) end", "transpile": true}
    """
    code = params.get('code')
    script_path = params.get('script_path')
    transpile = params.get('transpile', False)
    
    if not code and not script_path:
        return {"success": False, "error": "Missing code or script_path parameter"}
    
    if script_path:
        full_path = _validate_path(sandbox_path, script_path)
        if not os.path.exists(full_path):
            return {"success": False, "error": f"Script file not found: {script_path}"}
        with open(full_path, 'r', encoding='utf-8') as f:
            code = f.read()
    
    try:
        # Lua è¯­æ³•åˆ†æ
        issues = []
        warnings = []
        
        # 1. æ£€æŸ¥æ‹¬å·åŒ¹é…
        function_count = len([m for m in __import__('re').finditer(r'\bfunction\b', code)])
        end_count = code.count('end')
        if function_count > 0 and abs(function_count - end_count) > 2:
            issues.append(f"Possible 'end' mismatch (functions: {function_count}, ends: {end_count})")
        
        # 2. æ£€æŸ¥å­—ç¬¦ä¸²åŒ¹é…
        single_quotes = code.count("'") - code.count("\\'")
        double_quotes = code.count('"') - code.count('\\"')
        if single_quotes % 2 != 0:
            warnings.append("Unmatched single quotes")
        if double_quotes % 2 != 0:
            warnings.append("Unmatched double quotes")
        
        # 3. æ£€æŸ¥æ‹¬å·åŒ¹é…
        open_parens = code.count('(')
        close_parens = code.count(')')
        if open_parens != close_parens:
            issues.append(f"Parenthesis mismatch: {open_parens} open, {close_parens} close")
        
        # 4. æ£€æµ‹ä»£ç ç»“æ„
        has_function = 'function' in code
        has_loop = 'for ' in code or 'while ' in code
        has_conditional = 'if ' in code
        has_table = '{' in code and '}' in code
        
        result = {
            "success": True,
            "language": "lua",
            "syntax_valid": len(issues) == 0,
            "issues": issues,
            "warnings": warnings,
            "structure": {
                "has_function": has_function,
                "has_loop": has_loop,
                "has_conditional": has_conditional,
                "has_table": has_table,
                "lines": code.count('\n') + 1
            },
            "note": "Lua syntax analysis (execution requires native Lua interpreter)"
        }
        
        # å¦‚æœéœ€è¦ï¼Œè½¬æ¢ä¸º Python æ‰§è¡Œ
        if transpile:
            import re
            
            # ç®€å•çš„ Lua -> Python è½¬æ¢
            py_code = code
            
            # è½¬æ¢å‡½æ•°å®šä¹‰
            py_code = re.sub(r'function\s+(\w+)\s*\((.*?)\)', r'def \1(\2):', py_code)
            py_code = re.sub(r'local\s+function\s+(\w+)\s*\((.*?)\)', r'def \1(\2):', py_code)
            
            # è½¬æ¢ local å˜é‡
            py_code = re.sub(r'\blocal\s+', '', py_code)
            
            # è½¬æ¢ print
            py_code = re.sub(r'\bprint\s*\(', 'print(', py_code)
            
            # è½¬æ¢å­—ç¬¦ä¸²è¿æ¥
            py_code = re.sub(r'\.\.', ' + ', py_code)
            
            # è½¬æ¢ # æ“ä½œç¬¦ï¼ˆé•¿åº¦ï¼‰
            py_code = re.sub(r'#(\w+)', r'len(\1)', py_code)
            
            # è½¬æ¢ table è®¿é—®
            py_code = re.sub(r'(\w+)\[(\w+)\]', r'\1[\2]', py_code)
            
            # è½¬æ¢ ipairs/pairs å¾ªç¯ï¼ˆç®€åŒ–ï¼‰
            py_code = re.sub(r'for\s+(\w+)\s*,\s*(\w+)\s+in\s+ipairs\((\w+)\)\s+do', 
                           r'for \1, \2 in enumerate(\3):', py_code)
            py_code = re.sub(r'for\s+(\w+)\s*,\s*(\w+)\s+in\s+pairs\((\w+)\)\s+do', 
                           r'for \1, \2 in \3.items():', py_code)
            
            # è½¬æ¢ç®€å• for å¾ªç¯
            py_code = re.sub(r'for\s+(\w+)\s*=\s*(\d+),\s*(\d+)\s+do', 
                           r'for \1 in range(\2, \3 + 1):', py_code)
            
            # è½¬æ¢ while
            py_code = re.sub(r'while\s+(.+?)\s+do', r'while \1:', py_code)
            
            # è½¬æ¢ if
            py_code = re.sub(r'if\s+(.+?)\s+then', r'if \1:', py_code)
            py_code = re.sub(r'elseif\s+(.+?)\s+then', r'elif \1:', py_code)
            py_code = re.sub(r'\belse\b', 'else:', py_code)
            
            # ç§»é™¤ end å…³é”®å­—
            py_code = re.sub(r'\bend\b', '', py_code)
            
            # è½¬æ¢ nil
            py_code = re.sub(r'\bnil\b', 'None', py_code)
            
            # è½¬æ¢ and/or/not
            py_code = re.sub(r'\band\b', 'and', py_code)
            py_code = re.sub(r'\bor\b', 'or', py_code)
            py_code = re.sub(r'\bnot\b', 'not', py_code)
            
            # å°è¯•æ‰§è¡Œè½¬æ¢åçš„ä»£ç 
            try:
                exec_globals = {
                    'len': len,
                    'print': print,
                    'range': range,
                    'enumerate': enumerate
                }
                exec_locals = {}
                
                import io
                import sys
                old_stdout = sys.stdout
                stdout_capture = io.StringIO()
                sys.stdout = stdout_capture
                
                exec(compile(py_code, '<lua_transpiled>', 'exec'), exec_globals, exec_locals)
                
                sys.stdout = old_stdout
                output = stdout_capture.getvalue()
                
                result['transpiled_python'] = py_code
                result['execution_output'] = output
                result['note'] = 'Lua transpiled to Python and executed'
                
            except Exception as e:
                result['transpiled_python'] = py_code
                result['transpile_error'] = str(e)
                result['note'] = 'Lua transpiled but execution failed'
        
        return result
        
    except Exception as e:
        return {"success": False, "error": f"Lua analysis failed: {str(e)}"}


def _analyze_code(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»£ç åˆ†æä¸è´¨é‡æ£€æŸ¥ï¼ˆæ”¯æŒå¤šè¯­è¨€ï¼‰
    
    ç”¨äº AI è‡ªä¸¾å¼€å‘ï¼šè¯­æ³•æ£€æŸ¥ã€å¤æ‚åº¦åˆ†æã€ç”Ÿæˆè¡¥ä¸
    
    Params:
        - file_path: è¦åˆ†æçš„æ–‡ä»¶è·¯å¾„
        - language: è¯­è¨€ç±»å‹ (python/kotlin/javascript/lua)
        - operation: åˆ†æç±»å‹ (syntax/complexity/ast/patch)
        
    Returns:
        - syntax_valid: è¯­æ³•æ˜¯å¦æ­£ç¡®
        - issues: é—®é¢˜åˆ—è¡¨
        - metrics: ä»£ç åº¦é‡ï¼ˆè¡Œæ•°ã€å¤æ‚åº¦ç­‰ï¼‰
        - ast_dump: AST ç»“æ„ï¼ˆå¯é€‰ï¼‰
    """
    file_path = params.get('file_path')
    language = params.get('language', 'auto')
    operation = params.get('operation', 'syntax')
    
    if not file_path:
        return {"success": False, "error": "Missing file_path parameter"}
    
    try:
        full_path = _validate_path(sandbox_path, file_path)
        if not os.path.exists(full_path):
            return {"success": False, "error": f"File not found: {file_path}"}
        
        with open(full_path, 'r', encoding='utf-8') as f:
            code = f.read()
        
        # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
        if language == 'auto':
            ext = os.path.splitext(file_path)[1].lower()
            lang_map = {
                '.py': 'python',
                '.kt': 'kotlin',
                '.js': 'javascript',
                '.lua': 'lua',
                '.sh': 'shell',
                '.json': 'json',
                '.yaml': 'yaml',
                '.yml': 'yaml'
            }
            language = lang_map.get(ext, 'unknown')
        
        result = {
            "success": True,
            "file_path": file_path,
            "language": language,
            "file_size": len(code),
            "lines": code.count('\n') + 1
        }
        
        if language == 'python':
            # Python AST åˆ†æ
            import ast
            try:
                tree = ast.parse(code)
                result['syntax_valid'] = True
                
                if operation == 'complexity':
                    # ç®€å•çš„å¤æ‚åº¦åˆ†æ
                    func_count = len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)])
                    class_count = len([n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)])
                    import_count = len([n for n in ast.walk(tree) if isinstance(n, ast.Import)])
                    
                    result['metrics'] = {
                        'functions': func_count,
                        'classes': class_count,
                        'imports': import_count,
                        'complexity_score': func_count * 2 + class_count * 3
                    }
                
                if operation == 'ast':
                    # è¿”å› AST ç»“æ„ï¼ˆç®€åŒ–ç‰ˆï¼‰
                    result['ast_dump'] = ast.dump(tree, indent=2)[:5000]  # é™åˆ¶å¤§å°
                    
            except SyntaxError as e:
                result['syntax_valid'] = False
                result['syntax_error'] = f"Line {e.lineno}: {e.msg}"
                
        elif language == 'kotlin':
            # Kotlin åŸºç¡€åˆ†æï¼ˆæ–‡æœ¬å±‚é¢ï¼‰
            # å®Œæ•´çš„åˆ†æéœ€è¦ kotlincï¼Œè¿™é‡ŒåšåŸºç¡€æ£€æŸ¥
            issues = []
            
            # æ£€æŸ¥åŸºæœ¬è¯­æ³•ç»“æ„
            open_braces = code.count('{')
            close_braces = code.count('}')
            open_parens = code.count('(')
            close_parens = code.count(')')
            
            if open_braces != close_braces:
                issues.append(f"Brace mismatch: {open_braces} open, {close_braces} close")
            if open_parens != close_parens:
                issues.append(f"Parenthesis mismatch: {open_parens} open, {close_parens} close")
            
            # æ£€æŸ¥å¸¸è§å…³é”®å­—
            keywords = ['fun ', 'class ', 'val ', 'var ', 'import ', 'package ']
            found_keywords = [kw for kw in keywords if kw in code]
            
            result['syntax_valid'] = len(issues) == 0
            result['issues'] = issues
            result['detected_keywords'] = found_keywords
            result['note'] = 'Basic syntax check only (full validation requires kotlinc)'
            
        elif language == 'javascript':
            # JavaScript åŸºç¡€åˆ†æ
            issues = []
            
            # æ£€æŸ¥æ‹¬å·åŒ¹é…
            open_braces = code.count('{')
            close_braces = code.count('}')
            open_parens = code.count('(')
            close_parens = code.count(')')
            open_brackets = code.count('[')
            close_brackets = code.count(']')
            
            if open_braces != close_braces:
                issues.append(f"Brace mismatch: {open_braces} open, {close_braces} close")
            if open_parens != close_parens:
                issues.append(f"Parenthesis mismatch: {open_parens} open, {close_parens} close")
            if open_brackets != close_brackets:
                issues.append(f"Bracket mismatch: {open_brackets} open, {close_brackets} close")
            
            # æ£€æŸ¥å­—ç¬¦ä¸²å¼•å·åŒ¹é…ï¼ˆç®€å•æ£€æŸ¥ï¼‰
            single_quotes = code.count("'") - code.count("\\'")
            double_quotes = code.count('"') - code.count('\\"')
            
            # æ£€æŸ¥å¸¸è§è¯­æ³•é”™è¯¯
            if 'function function' in code:
                issues.append("Duplicate 'function' keyword")
            if 'const const' in code or 'let let' in code or 'var var' in code:
                issues.append("Duplicate declaration keyword")
            
            # æ£€æµ‹ä»£ç ç»“æ„
            has_function = 'function' in code or '=>' in code
            has_class = 'class ' in code
            has_async = 'async ' in code
            has_await = 'await ' in code
            
            result['syntax_valid'] = len(issues) == 0
            result['issues'] = issues
            result['structure'] = {
                'has_function': has_function,
                'has_class': has_class,
                'has_async': has_async,
                'has_await': has_await
            }
            result['note'] = 'Basic syntax check only (full validation requires Node.js)'
                
        elif language == 'lua':
            # Lua åŸºç¡€æ£€æŸ¥
            issues = []
            
            # æ£€æŸ¥ end å…³é”®å­—åŒ¹é…
            function_count = code.count('function')
            end_count = code.count('end')
            
            # ç²—ç•¥ä¼°è®¡ï¼ˆä¸å‡†ç¡®ï¼Œä½†ä½œä¸ºå¯å‘å¼æ£€æŸ¥ï¼‰
            if abs(function_count - end_count) > 2:
                issues.append(f"Possible 'end' mismatch (functions: {function_count}, ends: {end_count})")
            
            result['syntax_valid'] = len(issues) == 0
            result['issues'] = issues
            result['note'] = 'Basic heuristic check only'
        
        return result
        
    except Exception as e:
        return {"success": False, "error": f"Code analysis failed: {str(e)}"}


def _compile_check(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    è½»é‡çº§ç¼–è¯‘/è¯­æ³•éªŒè¯ï¼ˆå†…ç½®åœ¨æ²™ç®±ä¸­ï¼‰
    
    æ”¯æŒå¤šç§è¯­è¨€çš„è¯­æ³•æ£€æŸ¥ã€æ ¼å¼åŒ–ã€é£æ ¼éªŒè¯
    æ— éœ€å¤–éƒ¨ç¼–è¯‘å™¨ï¼Œä½¿ç”¨ Python ç”Ÿæ€å·¥å…·
    
    Params:
        - file_path: è¦æ£€æŸ¥çš„æ–‡ä»¶è·¯å¾„
        - language: è¯­è¨€ç±»å‹ (python/kotlin/java/shell/markdown/json/yaml)
        - check_type: æ£€æŸ¥ç±»å‹ (syntax/format/lint/allï¼Œé»˜è®¤ all)
        - fix: æ˜¯å¦è‡ªåŠ¨ä¿®å¤é—®é¢˜ï¼ˆé»˜è®¤ falseï¼‰
        
    Returns:
        - valid: æ˜¯å¦é€šè¿‡éªŒè¯
        - issues: é—®é¢˜åˆ—è¡¨ï¼ˆè¡Œå·ã€çº§åˆ«ã€æ¶ˆæ¯ï¼‰
        - fixed_content: ä¿®å¤åçš„å†…å®¹ï¼ˆå¦‚æœ fix=trueï¼‰
        - tool_used: ä½¿ç”¨çš„å·¥å…·
        
    Examples:
        {"file_path": "script.py", "language": "python", "check_type": "all"}
        {"file_path": "code.kt", "language": "kotlin", "fix": true}
    """
    file_path = params.get('file_path')
    language = params.get('language', 'auto')
    check_type = params.get('check_type', 'all')
    fix = params.get('fix', False)
    
    if not file_path:
        return {"success": False, "error": "Missing file_path parameter"}
    
    try:
        full_path = _validate_path(sandbox_path, file_path)
        if not os.path.exists(full_path):
            return {"success": False, "error": f"File not found: {file_path}"}
        
        with open(full_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
        
        # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
        if language == 'auto':
            ext = os.path.splitext(file_path)[1].lower()
            lang_map = {
                '.py': 'python',
                '.kt': 'kotlin',
                '.java': 'java',
                '.sh': 'shell',
                '.md': 'markdown',
                '.json': 'json',
                '.yaml': 'yaml',
                '.yml': 'yaml',
                '.toml': 'toml',
                '.lua': 'lua',
                '.js': 'javascript',
                '.css': 'css',
                '.html': 'html',
            }
            language = lang_map.get(ext, 'unknown')
        
        result = {
            "success": True,
            "file_path": file_path,
            "language": language,
            "check_type": check_type,
            "valid": True,
            "issues": [],
            "tool_used": None
        }
        
        # ========== Python ==========
        if language == 'python':
            issues = []
            
            # 1. è¯­æ³•æ£€æŸ¥ï¼ˆASTï¼‰
            if check_type in ('syntax', 'all'):
                try:
                    import ast
                    ast.parse(content)
                except SyntaxError as e:
                    issues.append({
                        "line": e.lineno or 1,
                        "column": e.offset or 0,
                        "level": "error",
                        "message": f"SyntaxError: {e.msg}",
                        "tool": "ast"
                    })
                    result['valid'] = False
            
            # 2. ä½¿ç”¨ ruffï¼ˆå¦‚æœå®‰è£…äº†ï¼‰
            if check_type in ('lint', 'all'):
                try:
                    import subprocess
                    import json as json_mod
                    
                    ruff_result = subprocess.run(
                        ['python', '-m', 'ruff', 'check', full_path, '--output-format=json'],
                        capture_output=True,
                        text=True,
                        timeout=30
                    )
                    
                    if ruff_result.stdout:
                        try:
                            ruff_issues = json_mod.loads(ruff_result.stdout)
                            for issue in ruff_issues:
                                issues.append({
                                    "line": issue.get('location', {}).get('row', 1),
                                    "column": issue.get('location', {}).get('column', 0),
                                    "level": issue.get('code', 'WARNING'),
                                    "message": issue.get('message', 'Unknown issue'),
                                    "tool": "ruff"
                                })
                                if issue.get('code', '').startswith('E'):
                                    result['valid'] = False
                        except:
                            pass
                            
                    result['tool_used'] = 'ruff'
                except (ImportError, subprocess.TimeoutExpired, FileNotFoundError):
                    # ruff ä¸å¯ç”¨ï¼Œå›é€€åˆ° pylint
                    try:
                        import subprocess
                        pylint_result = subprocess.run(
                            ['python', '-m', 'pylint', full_path, '--output-format=json'],
                            capture_output=True,
                            text=True,
                            timeout=30
                        )
                        
                        if pylint_result.stdout:
                            try:
                                import json as json_mod
                                pylint_issues = json_mod.loads(pylint_result.stdout)
                                for issue in pylint_issues:
                                    if issue.get('type') == 'error':
                                        issues.append({
                                            "line": issue.get('line', 1),
                                            "column": issue.get('column', 0),
                                            "level": "error",
                                            "message": issue.get('message', 'Unknown'),
                                            "tool": "pylint"
                                        })
                                        result['valid'] = False
                            except:
                                pass
                        result['tool_used'] = 'pylint'
                    except:
                        pass
            
            # 3. æ ¼å¼åŒ–æ£€æŸ¥/ä¿®å¤
            if check_type in ('format', 'all') or fix:
                try:
                    import subprocess
                    
                    if fix:
                        # ä½¿ç”¨ black æ ¼å¼åŒ–
                        black_result = subprocess.run(
                            ['python', '-m', 'black', full_path, '--quiet'],
                            capture_output=True,
                            text=True,
                            timeout=30
                        )
                        if black_result.returncode == 0:
                            with open(full_path, 'r', encoding='utf-8') as f:
                                result['fixed_content'] = f.read()
                            result['formatted'] = True
                    else:
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ ¼å¼åŒ–
                        black_check = subprocess.run(
                            ['python', '-m', 'black', '--check', full_path],
                            capture_output=True,
                            text=True,
                            timeout=30
                        )
                        if black_check.returncode != 0:
                            issues.append({
                                "line": 1,
                                "column": 0,
                                "level": "style",
                                "message": "Code needs formatting (run with fix=true)",
                                "tool": "black"
                            })
                except:
                    pass
        
        # ========== JSON ==========
        elif language == 'json':
            import json as json_mod
            try:
                json_mod.loads(content)
                result['valid'] = True
                result['tool_used'] = 'json'
            except json_mod.JSONDecodeError as e:
                issues.append({
                    "line": e.lineno or 1,
                    "column": e.colno or 0,
                    "level": "error",
                    "message": f"JSON Error: {e.msg}",
                    "tool": "json"
                })
                result['valid'] = False
        
        # ========== YAML ==========
        elif language == 'yaml':
            try:
                import yaml
                yaml.safe_load(content)
                result['valid'] = True
                result['tool_used'] = 'yaml'
            except yaml.YAMLError as e:
                if hasattr(e, 'problem_mark'):
                    mark = e.problem_mark
                    issues.append({
                        "line": mark.line + 1 if mark else 1,
                        "column": mark.column + 1 if mark else 0,
                        "level": "error",
                        "message": f"YAML Error: {e.problem}",
                        "tool": "yaml"
                    })
                else:
                    issues.append({
                        "line": 1,
                        "column": 0,
                        "level": "error",
                        "message": f"YAML Error: {str(e)}",
                        "tool": "yaml"
                    })
                result['valid'] = False
        
        # ========== TOML ==========
        elif language == 'toml':
            try:
                import tomllib  # Python 3.11+
                tomllib.loads(content)
                result['valid'] = True
                result['tool_used'] = 'tomllib'
            except ImportError:
                try:
                    import toml
                    toml.loads(content)
                    result['valid'] = True
                    result['tool_used'] = 'toml'
                except Exception as e:
                    issues.append({
                        "line": 1,
                        "column": 0,
                        "level": "error",
                        "message": f"TOML Error: {str(e)}",
                        "tool": "toml"
                    })
                    result['valid'] = False
            except Exception as e:
                issues.append({
                    "line": 1,
                    "column": 0,
                    "level": "error",
                    "message": f"TOML Error: {str(e)}",
                    "tool": "tomllib"
                })
                result['valid'] = False
        
        # ========== Shell ==========
        elif language == 'shell':
            # åŸºç¡€è¯­æ³•æ£€æŸ¥
            issues = []
            
            # æ£€æŸ¥å¸¸è§é”™è¯¯
            lines = content.split('\n')
            for i, line in enumerate(lines, 1):
                stripped = line.strip()
                
                # æ£€æŸ¥æœªé—­åˆçš„å¼•å·
                single_quotes = stripped.count("'") - stripped.count("\\'")
                double_quotes = stripped.count('"') - stripped.count('\\"')
                
                # ç®€å•å¯å‘å¼æ£€æŸ¥
                if 'if [' in stripped and '];' not in stripped and ' then' not in stripped:
                    issues.append({
                        "line": i,
                        "column": 0,
                        "level": "warning",
                        "message": "Possible missing 'then' or semicolon in if statement",
                        "tool": "shellcheck-lite"
                    })
                
                if stripped.startswith('function ') and '()' not in stripped:
                    # æ£€æŸ¥ function å®šä¹‰æ ¼å¼
                    pass
            
            # å°è¯•ç”¨ sh -n æ£€æŸ¥è¯­æ³•
            try:
                import subprocess
                check_result = subprocess.run(
                    ['/system/bin/sh', '-n', full_path],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                if check_result.returncode != 0:
                    issues.append({
                        "line": 1,
                        "column": 0,
                        "level": "error",
                        "message": f"Shell syntax error: {check_result.stderr}",
                        "tool": "sh"
                    })
                    result['valid'] = False
                else:
                    result['valid'] = len([i for i in issues if i['level'] == 'error']) == 0
            except:
                result['valid'] = len(issues) == 0
            
            result['tool_used'] = 'shellcheck-lite'
            result['issues'] = issues
        
        # ========== Kotlin/Java ==========
        elif language in ('kotlin', 'java'):
            issues = []
            ktlint_available = False
            
            # å°è¯•ä½¿ç”¨ ktlintï¼ˆKotlin é¦–é€‰ï¼‰
            if language == 'kotlin':
                ktlint_path = _get_ktlint_path(sandbox_path)
                
                if ktlint_path and os.path.exists(ktlint_path):
                    ktlint_available = True
                    
                    try:
                        # å‡†å¤‡ ktlint å‚æ•°
                        ktlint_args = [ktlint_path, full_path, '--reporter=json']
                        
                        if fix:
                            ktlint_args.append('--format')
                        
                        # è¿è¡Œ ktlint
                        ktlint_result = subprocess.run(
                            ktlint_args,
                            capture_output=True,
                            text=True,
                            timeout=60
                        )
                        
                        # è§£æ ktlint JSON è¾“å‡º
                        if ktlint_result.stdout:
                            try:
                                ktlint_issues = json_mod.loads(ktlint_result.stdout)
                                for issue in ktlint_issues:
                                    # ç¡®å®šé”™è¯¯çº§åˆ«
                                    rule_id = issue.get('ruleId', '')
                                    if rule_id in ['indent', 'no-wildcard-imports', 'colon-spacing']:
                                        level = 'error' if not fix else 'style'
                                    else:
                                        level = 'warning'
                                    
                                    issues.append({
                                        "line": issue.get('line', 1),
                                        "column": issue.get('column', 0),
                                        "level": level,
                                        "message": issue.get('detail', issue.get('message', 'Unknown issue')),
                                        "rule": rule_id,
                                        "tool": "ktlint"
                                    })
                                    
                                    if level == 'error':
                                        result['valid'] = False
                            except json_mod.JSONDecodeError:
                                # å¯èƒ½æ˜¯æ ¼å¼åŒ–çš„è¾“å‡ºï¼Œä¸æ˜¯ JSON
                                pass
                        
                        # å¦‚æœ fix=trueï¼Œè¯»å–ä¿®å¤åçš„æ–‡ä»¶
                        if fix and ktlint_result.returncode == 0:
                            with open(full_path, 'r', encoding='utf-8') as f:
                                result['fixed_content'] = f.read()
                            result['formatted'] = True
                        
                        result['tool_used'] = 'ktlint'
                        result['ktlint_version'] = os.path.basename(ktlint_path).replace('ktlint-', '')
                        
                    except Exception as e:
                        # ktlint è¿è¡Œå¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æ£€æŸ¥
                        issues.append({
                            "line": 1,
                            "column": 0,
                            "level": "warning",
                            "message": f"ktlint failed: {str(e)}, falling back to basic check",
                            "tool": "system"
                        })
                        ktlint_available = False
            
            # å¦‚æœæ²¡æœ‰ ktlint æˆ– ktlint å¤±è´¥ï¼Œè¿›è¡ŒåŸºç¡€æ£€æŸ¥
            if not ktlint_available:
                # æ‹¬å·åŒ¹é…
                open_braces = content.count('{')
                close_braces = content.count('}')
                open_parens = content.count('(')
                close_parens = content.count(')')
                
                if open_braces != close_braces:
                    issues.append({
                        "line": 1,
                        "column": 0,
                        "level": "error",
                        "message": f"Brace mismatch: {open_braces} open, {close_braces} close",
                        "tool": "syntax"
                    })
                    result['valid'] = False
                
                if open_parens != close_parens:
                    issues.append({
                        "line": 1,
                        "column": 0,
                        "level": "error",
                        "message": f"Parenthesis mismatch: {open_parens} open, {close_parens} close",
                        "tool": "syntax"
                    })
                    result['valid'] = False
                
                # æ£€æŸ¥å¸¸è§ Kotlin é—®é¢˜
                # 1. æ£€æŸ¥å‡½æ•°å‘½åï¼ˆåº”è¯¥æ˜¯ camelCaseï¼‰
                import re
                func_pattern = r'fun\s+([A-Z][a-zA-Z0-9]*)\s*\('
                bad_funcs = re.findall(func_pattern, content)
                for func in bad_funcs:
                    issues.append({
                        "line": 1,
                        "column": 0,
                        "level": "warning",
                        "message": f"Function '{func}' should start with lowercase (camelCase)",
                        "tool": "style-check"
                    })
                
                # 2. æ£€æŸ¥ç±»å‘½åï¼ˆåº”è¯¥æ˜¯ PascalCaseï¼‰
                class_pattern = r'class\s+([a-z][a-zA-Z0-9]*)'
                bad_classes = re.findall(class_pattern, content)
                for cls in bad_classes:
                    issues.append({
                        "line": 1,
                        "column": 0,
                        "level": "warning",
                        "message": f"Class '{cls}' should start with uppercase (PascalCase)",
                        "tool": "style-check"
                    })
                
                # 3. æ£€æµ‹å¯èƒ½çš„æœªä½¿ç”¨å¯¼å…¥ï¼ˆç®€å•å¯å‘å¼ï¼‰
                import_pattern = r'import\s+([\w.]+)'
                imports = re.findall(import_pattern, content)
                for imp in imports:
                    # è·å–æœ€åä¸€éƒ¨åˆ†ï¼ˆç±»åæˆ–å‡½æ•°åï¼‰
                    name = imp.split('.')[-1]
                    # ç²—ç•¥æ£€æŸ¥æ˜¯å¦è¢«ä½¿ç”¨
                    if name not in ['*'] and content.count(name) <= 1:  # åªåœ¨ import ä¸­å‡ºç°ä¸€æ¬¡
                        issues.append({
                            "line": 1,
                            "column": 0,
                            "level": "info",
                            "message": f"Import '{imp}' may be unused ( heuristic check)",
                            "tool": "unused-check"
                        })
                
                result['tool_used'] = 'syntax-check'
                result['note'] = 'Basic syntax check only. For deeper analysis, run: {"operation": "install_tool", "params": {"tool": "ktlint"}}'
            
            result['issues'] = issues
            if result['valid'] is None:
                result['valid'] = len([i for i in issues if i['level'] == 'error']) == 0
        
        # ========== Markdown ==========
        elif language == 'markdown':
            # Markdown åŸºç¡€æ£€æŸ¥
            issues = []
            
            # æ£€æŸ¥é“¾æ¥æ ¼å¼
            import re
            links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
            for text, url in links:
                if not url.startswith(('http://', 'https://', '#', './', '../', '/')):
                    issues.append({
                        "line": 1,
                        "column": 0,
                        "level": "warning",
                        "message": f"Suspicious link format: {url}",
                        "tool": "markdown-lint"
                    })
            
            # æ£€æŸ¥æ ‡é¢˜å±‚æ¬¡
            headers = re.findall(r'^(#{1,6})', content, re.MULTILINE)
            prev_level = 0
            for header in headers:
                level = len(header)
                if level > prev_level + 1 and prev_level > 0:
                    issues.append({
                        "line": 1,
                        "column": 0,
                        "level": "warning",
                        "message": f"Header level jumps from {prev_level} to {level}",
                        "tool": "markdown-lint"
                    })
                prev_level = level
            
            result['valid'] = len([i for i in issues if i['level'] == 'error']) == 0
            result['issues'] = issues
            result['tool_used'] = 'markdown-lint'
        
        # ========== å…¶ä»–è¯­è¨€ ==========
        else:
            result['valid'] = 'unknown'
            result['note'] = f"Language '{language}' not fully supported for compile_check"
        
        result['issues_count'] = len(result.get('issues', []))
        result['error_count'] = len([i for i in result.get('issues', []) if i.get('level') == 'error'])
        result['warning_count'] = len([i for i in result.get('issues', []) if i.get('level') == 'warning'])
        
        return result
        
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": f"Compile check failed: {str(e)}",
            "traceback": traceback.format_exc()
        }


# ========== å·¥å…·å®‰è£…ç®¡ç† ==========

# å·¥å…·ä¸‹è½½é…ç½®
TOOL_DOWNLOAD_URLS = {
    'ktlint': {
        'url': 'https://github.com/pinterest/ktlint/releases/download/{version}/ktlint',
        'default_version': '1.2.1',
        'executable': True,
    },
    'ktlint_android': {
        'url': 'https://github.com/pinterest/ktlint/releases/download/{version}/ktlint',
        'default_version': '1.2.1',
        'executable': True,
        'is_android_variant': True,
    }
}


def _install_tool(sandbox_path: str, params: Dict[str, Any]) -> Dict[str, Any]:
    """
    å®‰è£…å¼€å‘å·¥å…·åˆ°æ²™ç®±ï¼ˆktlint ç­‰ï¼‰
    
    æ”¯æŒçš„å·¥å…·ï¼š
    - ktlint: Kotlin ä»£ç æ£€æŸ¥å·¥å…· (~10MB)
    
    Params:
        - tool: å·¥å…·åç§° (ktlint)
        - version: ç‰ˆæœ¬å·ï¼ˆå¯é€‰ï¼Œé»˜è®¤æœ€æ–°ç¨³å®šç‰ˆï¼‰
        - force: å¼ºåˆ¶é‡æ–°å®‰è£…ï¼ˆé»˜è®¤ falseï¼‰
        
    Returns:
        - installed: æ˜¯å¦å®‰è£…æˆåŠŸ
        - path: å·¥å…·è·¯å¾„
        - version: å®é™…ç‰ˆæœ¬
        - size_mb: å·¥å…·å¤§å°
        
    Examples:
        {"tool": "ktlint"}
        {"tool": "ktlint", "version": "1.2.1"}
        {"tool": "ktlint", "force": true}
    """
    tool_name = params.get('tool')
    version = params.get('version')
    force = params.get('force', False)
    
    if not tool_name:
        return {"success": False, "error": "Missing tool parameter"}
    
    tool_name = tool_name.lower()
    
    if tool_name not in TOOL_DOWNLOAD_URLS:
        available = list(TOOL_DOWNLOAD_URLS.keys())
        return {"success": False, "error": f"Unknown tool: {tool_name}. Available: {available}"}
    
    config = TOOL_DOWNLOAD_URLS[tool_name]
    version = version or config['default_version']
    
    # å·¥å…·ç›®å½•
    tools_dir = os.path.join(sandbox_path, '.tools')
    tool_path = os.path.join(tools_dir, f"{tool_name}-{version}")
    
    try:
        # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
        if os.path.exists(tool_path) and not force:
            file_size = os.path.getsize(tool_path) / (1024 * 1024)
            return {
                "success": True,
                "data": f"Tool {tool_name} v{version} already installed",
                "installed": True,
                "path": tool_path,
                "version": version,
                "size_mb": round(file_size, 2),
                "cached": True
            }
        
        # åˆ›å»ºå·¥å…·ç›®å½•
        os.makedirs(tools_dir, exist_ok=True)
        
        # ä¸‹è½½å·¥å…·
        download_url = config['url'].format(version=version)
        
        # æ˜¾ç¤ºè¿›åº¦
        print(f"Downloading {tool_name} v{version}...")
        print(f"URL: {download_url}")
        
        # ä½¿ç”¨ requests ä¸‹è½½
        try:
            import requests
            response = requests.get(download_url, timeout=120, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            with open(tool_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            percent = (downloaded / total_size) * 100
                            if downloaded % (1024 * 1024) < 8192:  # æ¯ MB æ‰“å°ä¸€æ¬¡
                                print(f"Progress: {percent:.1f}% ({downloaded/(1024*1024):.1f} MB)")
            
            # è®¾ç½®å¯æ‰§è¡Œæƒé™
            if config.get('executable'):
                os.chmod(tool_path, 0o755)
            
            file_size = os.path.getsize(tool_path) / (1024 * 1024)
            
            # éªŒè¯å®‰è£…ï¼ˆç®€å•æµ‹è¯•ï¼‰
            if tool_name == 'ktlint':
                test_result = subprocess.run(
                    [tool_path, '--version'],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                actual_version = test_result.stdout.strip() if test_result.returncode == 0 else version
            else:
                actual_version = version
            
            return {
                "success": True,
                "data": f"Tool {tool_name} v{actual_version} installed successfully",
                "installed": True,
                "path": tool_path,
                "version": actual_version,
                "size_mb": round(file_size, 2),
                "cached": False
            }
            
        except requests.RequestException as e:
            # æ¸…ç†å¤±è´¥çš„ä¸‹è½½
            if os.path.exists(tool_path):
                os.remove(tool_path)
            return {"success": False, "error": f"Download failed: {str(e)}"}
            
    except Exception as e:
        # æ¸…ç†å¤±è´¥çš„å®‰è£…
        if os.path.exists(tool_path):
            try:
                os.remove(tool_path)
            except:
                pass
        return {"success": False, "error": f"Installation failed: {str(e)}"}


def _get_ktlint_path(sandbox_path: str, version: str = None) -> Optional[str]:
    """è·å– ktlint è·¯å¾„ï¼Œå¦‚æœæœªå®‰è£…è¿”å› None"""
    tools_dir = os.path.join(sandbox_path, '.tools')
    
    if version:
        tool_path = os.path.join(tools_dir, f"ktlint-{version}")
        if os.path.exists(tool_path):
            return tool_path
    else:
        # æŸ¥æ‰¾ä»»æ„ç‰ˆæœ¬
        import glob
        pattern = os.path.join(tools_dir, 'ktlint-*')
        matches = glob.glob(pattern)
        if matches:
            # è¿”å›æœ€æ–°ç‰ˆæœ¬ï¼ˆæŒ‰æ–‡ä»¶åæ’åºï¼‰
            return sorted(matches)[-1]
    
    return None


# å…¼å®¹æ—§æ¥å£
run = execute
